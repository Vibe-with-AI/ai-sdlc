This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-05-19T16:36:13.979Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.cursor/
  rules/
    ai_sdlc.mdc
.repomix/
  bundles.json
ai_sdlc/
  commands/
    __init__.py
    done.py
    init.py
    new.py
    next.py
    status.py
  cli.py
  utils.py
doing/
  hello-world-test/
    01-idea-hello-world-test.md
    tasks.md
prompts/
  01-idea-prompt.md
  02-prd-prompt.md
  03-prd-plus-prompt.md
  04-architecture-prompt.md
  05-systems-patterns.md
  06-tasks-prompt.md
  07-tests.md
tests/
  integration/
    test_cli_flow.py
  unit/
    test_init_command.py
    test_utils.py
  __init__.py
  conftest.py
.aisdlc
.aisdlc.lock
.gitignore
.pyrightconfig.json
CHANGELOG.md
LICENSE
pyproject.toml
README.md
RELEASE_NOTES.md
uv.lock

================================================================
Files
================================================================

================
File: .cursor/rules/ai_sdlc.mdc
================
---
description: 
globs: 
alwaysApply: false
---
# Cursor System Rule: Operating an **AI-SDLC** Workstream

**Role:** Act as the project‚Äôs *automation agent*, driving each lifecycle step by invoking the `aisdlc` CLI from the repo root and managing prompt-driven file generation.  
**Scope:** Build *features* with the 7-step chain (`01-idea` ‚Üí `07-tests`). **Do NOT** modify the AI-SDLC tool itself.

---

## 1. Bootstrap (run once per repo)

| Condition | Command | Result |
|-----------|---------|--------|
| `.aisdlc` or `prompts/` missing | `aisdlc init` | Scaffolds rulebook & prompt templates. |

---

## 2. Start a Feature Workstream

| Trigger | Command | Next Action |
|---------|---------|-------------|
| User describes a new idea | `aisdlc new "<Idea title>"` | Open `doing/<slug>/01-idea-<slug>.md` for the user to flesh out **Problem / Solution / Rabbit Holes**. |

---

## 3. Advance Through Steps

After the user saves the current step‚Äôs markdown file:

1. Run `aisdlc next`.  
2. Cursor reads:  
   * **Previous file** (to embed as `<prev_step> ‚Ä¶ </prev_step>`)  
   * **Prompt** in `prompts/<next_step>-prompt.md`  
3. Cursor generates `doing/<slug>/<next_step>-<slug>.md` with the prompt‚Äôs expected markdown template.  
4. Update `.aisdlc.lock` automatically.  
5. Notify the user to review / tweak, then repeat **3.1‚Äì3.5** until all 7 steps complete.

**Step order**

| # | Code | Purpose |
|:-:|------|---------|
| 1 | 01-idea         | Raw pitch & exploration |
| 2 | 02-prd          | Product Requirements |
| 3 | 03-prd-plus     | Critique & risk surfacing |
| 4 | 04-architecture | Proposed file tree & patterns |
| 5 | 05-system-patterns | Canonical architecture doc |
| 6 | 06-tasks        | Atomic task list |
| 7 | 07-tests        | Unit / integration test suites |

---

## 4. Track Progress

* `aisdlc status` ‚Äì show current step and checklist bar for each active workstream.

---

## 5. Finish & Archive

When `07-tests-<slug>.md` is generated and accepted:

| Command | Result |
|---------|--------|
| `aisdlc done` | Validates all seven files exist ‚Üí moves folder from `doing/` ‚Üí `done/`, clears `.aisdlc.lock`. |

---

## 6. Safety Guards

* **Never** skip a step or alter the prescribed markdown template.  
* Abort with a clear error if any expected file or prompt is missing.  
* Only interact with files inside the active `doing/<slug>/` folder and read-only `prompts/`.

================
File: .repomix/bundles.json
================
{
  "bundles": {}
}

================
File: ai_sdlc/commands/__init__.py
================
"""
Sub-commands for the `aisdlc` CLI.

Having this file makes `ai_sdlc.commands.*` a real Python package so
editors like Pyright/Pylance can resolve the imports.
"""

================
File: ai_sdlc/commands/done.py
================
"""`aisdlc done` ‚Äì validate finished stream and archive it."""

import sys
import shutil
from pathlib import Path

from ai_sdlc.utils import ROOT, load_config, read_lock, write_lock

def run_done() -> None:
    conf  = load_config()
    steps = conf["steps"]
    lock  = read_lock()
    if not lock:
        print("‚ùå  No active workstream.")
        return
    slug = lock["slug"]
    if lock["current"] != steps[-1]:
        print("‚ùå  Workstream not finished yet. Complete all steps before archiving.")
        return
    workdir = ROOT / conf["active_dir"] / slug
    missing = [s for s in steps if not (workdir / f"{s}-{slug}.md").exists()]
    if missing:
        print("‚ùå  Missing files:", ", ".join(missing))
        return
    dest = ROOT / conf["done_dir"] / slug
    try:
        shutil.move(str(workdir), dest)
        write_lock({})
        print(f"üéâ  Archived to {dest}")
    except OSError as e:
        print(f"‚ùå  Error archiving work-stream '{slug}': {e}")
        sys.exit(1)

================
File: ai_sdlc/commands/init.py
================
"""`aisdlc init` ‚Äì scaffold baseline folders & lock."""

from pathlib import Path

from ai_sdlc.utils import ROOT, write_lock


def run_init() -> None:
    """Create default folders and empty lock."""
    for folder in ("doing", "done"):
        (ROOT / folder).mkdir(exist_ok=True)

    write_lock({})
    print("‚úÖ  AI-SDLC initialized ‚Äì ready for `aisdlc new \"Your idea\"`")

================
File: ai_sdlc/commands/new.py
================
"""`aisdlc new` ‚Äì start a work-stream from an idea title."""

from __future__ import annotations

import datetime
import sys
from pathlib import Path

from ai_sdlc.utils import ROOT, slugify, write_lock


def run_new(args: list[str]) -> None:
    """Create the work-stream folder and first markdown file."""
    if not args:
        print("Usage: aisdlc new \"Idea title\"")
        sys.exit(1)

    idea_text = " ".join(args)
    slug = slugify(idea_text)

    workdir = ROOT / "doing" / slug
    if workdir.exists():
        print(f"‚ùå  Work-stream '{slug}' already exists.")
        sys.exit(1)

    try:
        workdir.mkdir(parents=True)
        idea_file = workdir / f"01-idea-{slug}.md"
        idea_file.write_text(
            f"# {idea_text}\n\n## Problem\n\n## Solution\n\n## Rabbit Holes\n",
        )

        write_lock(
            {
                "slug": slug,
                "current": "01-idea",
                "created": datetime.datetime.utcnow().isoformat(),
            },
        )
        print(f"‚úÖ  Created {idea_file}.  Fill it out, then run `aisdlc next`.")
    except OSError as e:
        print(f"‚ùå  Error creating work-stream files for '{slug}': {e}")
        sys.exit(1)

================
File: ai_sdlc/commands/next.py
================
"""`aisdlc next` ‚Äì generate the next lifecycle file via Cursor agent."""

from __future__ import annotations

import subprocess
import sys
import tempfile
from pathlib import Path

from ai_sdlc.utils import ROOT, load_config, read_lock, write_lock

# Define a reasonable timeout for cursor agent calls
CURSOR_AGENT_TIMEOUT = 300  # 5 minutes in seconds

PLACEHOLDER = "<prev_step></prev_step>"

def run_next() -> None:
    conf = load_config()
    steps = conf["steps"]
    lock = read_lock()

    if not lock:
        print("‚ùå  No active workstream. Run `aisdlc new` first.")
        return

    slug = lock["slug"]
    idx  = steps.index(lock["current"])
    if idx + 1 >= len(steps):
        print("üéâ  All steps complete. Run `aisdlc done` to archive.")
        return

    prev_step = steps[idx]
    next_step = steps[idx + 1]

    workdir = ROOT / conf["active_dir"] / slug
    prev_file   = workdir / f"{prev_step}-{slug}.md"
    prompt_file = ROOT / conf["prompt_dir"] / f"{next_step}-prompt.md"
    next_file   = workdir / f"{next_step}-{slug}.md"

    if not prev_file.exists():
        print(f"‚ùå  Expected file {prev_file} not found.")
        return
    if not prompt_file.exists():
        print(f"‚ùå  Prompt file {prompt_file} missing.")
        return

    print(f"‚ÑπÔ∏è  Reading previous step from: {prev_file}")
    prev_step_content = prev_file.read_text()
    print(f"‚ÑπÔ∏è  Reading prompt template from: {prompt_file}")
    prompt_template_content = prompt_file.read_text()
    
    merged_prompt = prompt_template_content.replace(PLACEHOLDER, prev_step_content)
    tmp_prompt_path_str = None  # Initialize for finally block
    try:
        with tempfile.NamedTemporaryFile(mode="w+", delete=False, suffix=".md", encoding="utf-8") as tmp_file_obj:
            tmp_prompt_path_str = tmp_file_obj.name
            tmp_file_obj.write(merged_prompt)

        print(f"üß†  Calling Cursor agent for step {next_step} ‚Ä¶")
        print(f"   Using temporary prompt file: {tmp_prompt_path_str}")
        try:
            output = subprocess.check_output(
                ["cursor", "agent", "--file", tmp_prompt_path_str],
                text=True,
                timeout=CURSOR_AGENT_TIMEOUT,
            )
    except subprocess.CalledProcessError as e:
        print(f"‚ùå  Cursor agent failed with exit code {e.returncode}.")
        if e.stdout: print(f"Stdout:\n{e.stdout}")
        if e.stderr: print(f"Stderr:\n{e.stderr}")
        sys.exit(1)
    except subprocess.TimeoutExpired:
        print(f"‚ùå  Cursor agent timed out after {CURSOR_AGENT_TIMEOUT} seconds.")
        sys.exit(1)
    finally:
        if tmp_prompt_path_str and Path(tmp_prompt_path_str).exists():
            Path(tmp_prompt_path_str).unlink()

    print(f"‚ÑπÔ∏è  Cursor agent finished. Writing output to: {next_file}")
    try:
        next_file.write_text(output)
        lock["current"] = next_step
        write_lock(lock)
        print(f"‚úÖ  Wrote {next_file}")
    except OSError as e:
        print(f"‚ùå  Error writing output to {next_file}: {e}")
        sys.exit(1)

================
File: ai_sdlc/commands/status.py
================
# ai_sdlc/commands/status.py
"""`aisdlc status` ‚Äì show progress through lifecycle steps."""

from ai_sdlc.utils import load_config, read_lock


def run_status() -> None:
    conf  = load_config()
    steps = conf["steps"]
    lock  = read_lock()
    print("Active workstreams\n------------------")
    if not lock:
        print("none ‚Äì create one with `aisdlc new`")
        return
    slug   = lock["slug"]
    cur    = lock["current"]
    idx    = steps.index(cur)
    bar    = " ‚ñ∏ ".join([("‚úÖ" if i <= idx else "‚òê") + s[2:] for i, s in enumerate(steps)])
    print(f"{slug:20} {cur:12} {bar}")

================
File: ai_sdlc/cli.py
================
#!/usr/bin/env python
"""Entry-point for the `aisdlc` CLI."""

from __future__ import annotations

import sys
from importlib import import_module
from typing import Callable, Dict

_COMMANDS: Dict[str, str] = {
    "init": "ai_sdlc.commands.init:run_init",
    "new": "ai_sdlc.commands.new:run_new",
    "next": "ai_sdlc.commands.next:run_next",
    "status": "ai_sdlc.commands.status:run_status",
    "done": "ai_sdlc.commands.done:run_done",
}


def _resolve(dotted: str) -> Callable[..., None]:
    """Import `"module:function"` and return the function object."""
    module_name, func_name = dotted.split(":")
    module = import_module(module_name)
    return getattr(module, func_name)


def main() -> None:  # noqa: D401
    """Run the requested sub-command."""
    cmd, *args = sys.argv[1:] or ["--help"]
    if cmd not in _COMMANDS:
        valid = "|".join(_COMMANDS.keys())
        print(f"Usage: aisdlc [{valid}]")
        sys.exit(1)

    handler = _resolve(_COMMANDS[cmd])
    handler(args) if args else handler()


if __name__ == "__main__":
    main()

================
File: ai_sdlc/utils.py
================
"""Shared helpers."""

from __future__ import annotations

from pathlib import Path
import json
import re
import sys
import unicodedata

def find_project_root() -> Path:
    """Find project root by searching for .aisdlc file in current and parent directories."""
    current_dir = Path.cwd()
    for parent in [current_dir] + list(current_dir.parents):
        if (parent / ".aisdlc").exists():
            return parent
    # Fallback or error
    print("Error: .aisdlc not found. Ensure you are in an ai-sdlc project directory.")
    sys.exit(1)

ROOT = find_project_root()

# --- TOML loader (Python ‚â•3.11 stdlib) --------------------------------------
try:
    import tomllib as _toml  # Python 3.11+
except ModuleNotFoundError:  # pragma: no cover ‚Äì fallback for < 3.11
    import tomli as _toml    # noqa: D401  # `uv pip install tomli`


def load_config() -> dict:
    cfg_path = ROOT / ".aisdlc"
    if not cfg_path.exists():
        raise FileNotFoundError(".aisdlc manifest missing ‚Äì run `aisdlc init`.")
    try:
        return _toml.loads(cfg_path.read_text())
    except _toml.TOMLDecodeError as e:
        print(f"‚ùå Error: '.aisdlc' configuration file is corrupted: {e}")
        print("Please fix the .aisdlc file or run 'aisdlc init' in a new directory.")
        import sys
        sys.exit(1)


def slugify(text: str) -> str:
    """kebab-case ascii only"""
    slug = unicodedata.normalize("NFKD", text).encode("ascii", "ignore").decode()
    slug = re.sub(r"[^a-zA-Z0-9]+", "-", slug).strip("-").lower()
    return slug or "idea"


def read_lock() -> dict:
    path = ROOT / ".aisdlc.lock"
    if not path.exists():
        return {}
    try:
        return json.loads(path.read_text())
    except json.JSONDecodeError:
        print("‚ö†Ô∏è  Warning: '.aisdlc.lock' file is corrupted or not valid JSON. Treating as empty.")
        return {}


def write_lock(data: dict) -> None:
    (ROOT / ".aisdlc.lock").write_text(json.dumps(data, indent=2))

================
File: doing/hello-world-test/01-idea-hello-world-test.md
================
# hello world test

## Problem

## Solution

## Rabbit Holes

================
File: doing/hello-world-test/tasks.md
================
Okay, here's a detailed task list based on the previous review, including specific paths, file names, actionables, and code snippets/diffs where appropriate.

---

## Task List for `ai-sdlc` Improvements

### 1. Error Handling and Robustness

**Task 1.1: Improve Cursor Agent Error Handling in `ai_sdlc/commands/next.py`**

*   **File:** `ai_sdlc/commands/next.py`
*   **Actionable:** Enhance the error message when the `cursor agent` subprocess fails to include `stdout` and `stderr` for better debugging.
*   **Details:**
    ```diff
    --- a/ai_sdlc/commands/next.py
    +++ b/ai_sdlc/commands/next.py
    @@ -30,7 +30,9 @@
                 text=True,
             )
         except subprocess.CalledProcessError as e:
    -        print("‚ùå  Cursor agent failed:", e)
    +        print(f"‚ùå  Cursor agent failed with exit code {e.returncode}.")
    +        if e.stdout: print(f"Stdout:\n{e.stdout}")
    +        if e.stderr: print(f"Stderr:\n{e.stderr}")
             sys.exit(1)

         next_file.write_text(output)

    ```

**Task 1.2: Add Timeout to Cursor Agent Call in `ai_sdlc/commands/next.py`**

*   **File:** `ai_sdlc/commands/next.py`
*   **Actionable:** Add a `timeout` to the `subprocess.check_output` call to prevent indefinite hangs. Catch `subprocess.TimeoutExpired`.
*   **Details:**
    ```python
    # ai_sdlc/commands/next.py
    # Define a reasonable timeout, e.g., 5 minutes (300 seconds)
    CURSOR_AGENT_TIMEOUT = 300
    ```
    ```diff
    --- a/ai_sdlc/commands/next.py
    +++ b/ai_sdlc/commands/next.py
    @@ -27,12 +27,17 @@
     try:
         output = subprocess.check_output(
             ["cursor", "agent", "--file", str(tmp_prompt)],
             text=True,
    +        timeout=CURSOR_AGENT_TIMEOUT, # Add timeout
         )
     except subprocess.CalledProcessError as e:
    -    print("‚ùå  Cursor agent failed:", e)
    +    print(f"‚ùå  Cursor agent failed with exit code {e.returncode}.")
    +    if e.stdout: print(f"Stdout:\n{e.stdout}")
    +    if e.stderr: print(f"Stderr:\n{e.stderr}")
    +    sys.exit(1)
    +except subprocess.TimeoutExpired:
    +    print(f"‚ùå  Cursor agent timed out after {CURSOR_AGENT_TIMEOUT} seconds.")
         sys.exit(1)

     next_file.write_text(output)
    ```

**Task 1.3: Add File I/O Error Handling in Commands**

*   **Files:** `ai_sdlc/commands/new.py`, `ai_sdlc/commands/done.py`, `ai_sdlc/commands/next.py`, `ai_sdlc/utils.py` (for `write_lock`)
*   **Actionable:** Wrap file system operations (`write_text`, `shutil.move`, `mkdir`) in `try...except OSError` blocks.
*   **Details (Example for `ai_sdlc/commands/new.py`):**
    ```diff
    --- a/ai_sdlc/commands/new.py
    +++ b/ai_sdlc/commands/new.py
    @@ -16,19 +16,26 @@
         print(f"‚ùå  Work-stream '{slug}' already exists.")
         sys.exit(1)

    -    workdir.mkdir(parents=True)
    -    idea_file = workdir / f"01-idea-{slug}.md"
    -    idea_file.write_text(
    -        f"# {idea_text}\n\n## Problem\n\n## Solution\n\n## Rabbit Holes\n",
    -    )
    +    try:
    +        workdir.mkdir(parents=True)
    +        idea_file = workdir / f"01-idea-{slug}.md"
    +        idea_file.write_text(
    +            f"# {idea_text}\n\n## Problem\n\n## Solution\n\n## Rabbit Holes\n",
    +        )
    +
    +        write_lock(
    +            {
    +                "slug": slug,
    +                "current": "01-idea",
    +                "created": datetime.datetime.utcnow().isoformat(),
    +            },
    +        )
    +        print(f"‚úÖ  Created {idea_file}.  Fill it out, then run `aisdlc next`.")
    +    except OSError as e:
    +        print(f"‚ùå  Error creating work-stream files for '{slug}': {e}")
    +        # Potentially attempt cleanup if partial files were created
    +        sys.exit(1)

    -    write_lock(
    -        {
    -            "slug": slug,
    -            "current": "01-idea",
    -            "created": datetime.datetime.utcnow().isoformat(),
    -        },
    -    )
    -    print(f"‚úÖ  Created {idea_file}.  Fill it out, then run `aisdlc next`.")
    ```
    *   **Apply similar `try...except OSError` blocks to:**
        *   `ai_sdlc/commands/done.py`: around `shutil.move` and `write_lock`.
        *   `ai_sdlc/commands/next.py`: around `next_file.write_text` and `write_lock`.
        *   `ai_sdlc/utils.py`: in `write_lock` around `write_text`.

**Task 1.4: Add Lock File Corruption Handling in `ai_sdlc/utils.py`**

*   **File:** `ai_sdlc/utils.py`
*   **Actionable:** Catch `json.JSONDecodeError` in `read_lock()` and provide a helpful message.
*   **Details:**
    ```diff
    --- a/ai_sdlc/utils.py
    +++ b/ai_sdlc/utils.py
    @@ -24,7 +24,13 @@

 def read_lock() -> dict:
     path = ROOT / ".aisdlc.lock"
    -    return json.loads(path.read_text()) if path.exists() else {}
    +    if not path.exists():
    +        return {}
    +    try:
    +        return json.loads(path.read_text())
    +    except json.JSONDecodeError:
    +        print("‚ö†Ô∏è  Warning: '.aisdlc.lock' file is corrupted or not valid JSON. Treating as empty.")
    +        return {} # Or sys.exit(1) if strictness is preferred


 def write_lock(data: dict) -> None:
    ```

**Task 1.5: Add Config File Corruption Handling in `ai_sdlc/utils.py`**

*   **File:** `ai_sdlc/utils.py`
*   **Actionable:** Catch `_toml.TOMLDecodeError` (or equivalent for `tomli`) in `load_config()`.
*   **Details:**
    ```diff
    --- a/ai_sdlc/utils.py
    +++ b/ai_sdlc/utils.py
    @@ -14,7 +14,12 @@
     cfg_path = ROOT / ".aisdlc"
     if not cfg_path.exists():
         raise FileNotFoundError(".aisdlc manifest missing ‚Äì run `aisdlc init`.")
    -    return _toml.loads(cfg_path.read_text())
    +    try:
    +        return _toml.loads(cfg_path.read_text())
    +    except _toml.TOMLDecodeError as e: # For tomllib, it's tomllib.TOMLDecodeError
    +        print(f"‚ùå Error: '.aisdlc' configuration file is corrupted: {e}")
    +        print("Please fix the .aisdlc file or run 'aisdlc init' in a new directory.")
    +        sys.exit(1) # Added import sys at the top of utils.py


 def slugify(text: str) -> str:
    ```
    *   Ensure `import sys` is added to `ai_sdlc/utils.py`.

---

### 2. Temporary File Management

**Task 2.1: Implement Secure Temporary File Handling in `ai_sdlc/commands/next.py`**

*   **File:** `ai_sdlc/commands/next.py`
*   **Actionable:** Replace hardcoded `/tmp/aisdlc_prompt.md` with `tempfile.NamedTemporaryFile`.
*   **Details:**
    ```diff
    --- a/ai_sdlc/commands/next.py
    +++ b/ai_sdlc/commands/next.py
    @@ -2,6 +2,7 @@
     from __future__ import annotations

     import subprocess
    +import tempfile
     import sys
     from pathlib import Path

    @@ -22,22 +23,30 @@
         return

     merged_prompt = prompt_file.read_text().replace(PLACEHOLDER, prev_file.read_text())
    -    tmp_prompt = Path("/tmp/aisdlc_prompt.md")
    -    tmp_prompt.write_text(merged_prompt)
    +    tmp_prompt_path_str = None # Initialize for finally block
    +    try:
    +        with tempfile.NamedTemporaryFile(mode="w+", delete=False, suffix=".md", encoding="utf-8") as tmp_file_obj:
    +            tmp_prompt_path_str = tmp_file_obj.name
    +            tmp_file_obj.write(merged_prompt)

    -    print(f"üß†  Calling Cursor agent for step {next_step} ‚Ä¶")
    -    try:
-        output = subprocess.check_output(
    -            ["cursor", "agent", "--file", str(tmp_prompt)],
    -            text=True,
    -        )
    -    except subprocess.CalledProcessError as e:
    -        print("‚ùå  Cursor agent failed:", e)
    -        sys.exit(1)
    +        print(f"üß†  Calling Cursor agent for step {next_step} ‚Ä¶")
    +        try:
    +            output = subprocess.check_output(
    +                ["cursor", "agent", "--file", tmp_prompt_path_str],
    +                text=True,
    +                timeout=CURSOR_AGENT_TIMEOUT,
    +            )
    +        except subprocess.CalledProcessError as e:
    +            print(f"‚ùå  Cursor agent failed with exit code {e.returncode}.")
    +            if e.stdout: print(f"Stdout:\n{e.stdout}")
    +            if e.stderr: print(f"Stderr:\n{e.stderr}")
    +            sys.exit(1)
    +        except subprocess.TimeoutExpired:
    +            print(f"‚ùå  Cursor agent timed out after {CURSOR_AGENT_TIMEOUT} seconds.")
    +            sys.exit(1)
    +    finally:
    +        if tmp_prompt_path_str and Path(tmp_prompt_path_str).exists():
    +            Path(tmp_prompt_path_str).unlink()

     next_file.write_text(output)
     lock["current"] = next_step

    ```

---

### 3. Configuration and Constants

**Task 3.1: (Optional/Consideration) Robust Project Root Detection**

*   **File:** `ai_sdlc/utils.py`
*   **Actionable:** (Consider for future enhancement) Modify `ROOT` detection to search upwards for `.aisdlc` if not found in `Path.cwd()`.
*   **Details:** This is more involved. For now, ensure documentation clearly states the CLI must be run from the repo root. If implemented, the logic would look something like:
    ```python
    # ai_sdlc/utils.py
    def find_project_root() -> Path:
        current_dir = Path.cwd()
        for parent in [current_dir] + list(current_dir.parents):
            if (parent / ".aisdlc").exists():
                return parent
        # Fallback or error
        print("Error: .aisdlc not found. Ensure you are in an ai-sdlc project directory.")
        sys.exit(1)

    ROOT = find_project_root()
    ```
    This change would affect how `ROOT` is defined and used globally.

---

### 4. Testing Strategy for `ai-sdlc`

**Task 4.1: Set up Pytest and Basic Test Structure**

*   **Files:** `pyproject.toml`, `tests/conftest.py` (new), `tests/__init__.py` (new)
*   **Actionable:**
    1.  Add `pytest` and `pytest-mock` to `dev` dependencies in `pyproject.toml`.
        ```toml
        # pyproject.toml
        [project.optional-dependencies]
        dev = [
            "pytest>=7.0",
            "pytest-mock>=3.0",
            # Add other dev tools like ruff, pyright if not already there
        ]
        ```
    2.  Create a `tests/` directory at the project root.
    3.  Create `tests/__init__.py` (can be empty).
    4.  Create `tests/conftest.py` for shared fixtures (e.g., a temporary directory for tests).
        ```python
        # tests/conftest.py
        import pytest
        import tempfile
        from pathlib import Path
        import shutil

        @pytest.fixture
        def temp_project_dir(tmp_path: Path):
            """Creates a temporary directory simulating a project root."""
            # tmp_path is a pytest fixture providing a temporary directory unique to the test
            # For more complex setups, you might copy baseline files here
            return tmp_path
        ```

**Task 4.2: Create Unit Tests for `ai_sdlc/utils.py`**

*   **File:** `tests/unit/test_utils.py` (new)
*   **Actionable:** Write unit tests for `slugify`, `load_config` (mocking file content), `read_lock`, `write_lock`.
*   **Details (Example for `slugify` and `load_config`):**
    ```python
    # tests/unit/test_utils.py
    import pytest
    from pathlib import Path
    import json
    from ai_sdlc import utils

    # Mock tomllib for testing load_config if it's not the stdlib version
    # Or ensure your test environment has the correct Python version / tomli installed

    def test_slugify():
        assert utils.slugify("Hello World!") == "hello-world"
        assert utils.slugify("  Test Slug with Spaces  ") == "test-slug-with-spaces"
        assert utils.slugify("Special!@#Chars") == "special-chars"
        assert utils.slugify("") == "idea" # As per current implementation

    def test_load_config_success(temp_project_dir: Path, mocker):
        mock_aisdlc_content = """
        version = "0.1.0"
        steps = ["01-idea", "02-prd"]
        prompt_dir = "prompts"
        """
        aisdlc_file = temp_project_dir / ".aisdlc"
        aisdlc_file.write_text(mock_aisdlc_content)

        mocker.patch('ai_sdlc.utils.ROOT', temp_project_dir) # Ensure ROOT points to test dir

        config = utils.load_config()
        assert config["version"] == "0.1.0"
        assert config["steps"] == ["01-idea", "02-prd"]

    def test_load_config_missing(temp_project_dir: Path, mocker):
        mocker.patch('ai_sdlc.utils.ROOT', temp_project_dir)
        with pytest.raises(FileNotFoundError, match="manifest missing"):
            utils.load_config()

    def test_load_config_corrupted(temp_project_dir: Path, mocker):
        aisdlc_file = temp_project_dir / ".aisdlc"
        aisdlc_file.write_text("this is not valid toml content {") # Corrupted TOML
        mocker.patch('ai_sdlc.utils.ROOT', temp_project_dir)
        mocker.patch('sys.exit') # Prevent test suite from exiting

        with pytest.raises(SystemExit): # Or check for printed error message
             utils.load_config()
        utils.sys.exit.assert_called_once_with(1)


    def test_read_write_lock(temp_project_dir: Path, mocker):
        mocker.patch('ai_sdlc.utils.ROOT', temp_project_dir)
        lock_data = {"slug": "test-slug", "current": "01-idea"}

        # Test write_lock
        utils.write_lock(lock_data)
        lock_file = temp_project_dir / ".aisdlc.lock"
        assert lock_file.exists()
        assert json.loads(lock_file.read_text()) == lock_data

        # Test read_lock
        read_data = utils.read_lock()
        assert read_data == lock_data

        # Test read_lock when file doesn't exist
        lock_file.unlink()
        assert utils.read_lock() == {}

        # Test read_lock with corrupted JSON
        lock_file.write_text("not json {")
        # Capture stdout to check for warning, or modify read_lock to raise on corruption for testing
        assert utils.read_lock() == {} # Assuming it returns {} on corruption as per Task 1.4
    ```

**Task 4.3: Create Unit Tests for `ai_sdlc/commands/*` (mocking dependencies)**

*   **Files:** `tests/unit/test_init_command.py`, `tests/unit/test_new_command.py`, etc.
*   **Actionable:** For each command, test its `run_*` function. Mock file system operations (`Path.mkdir`, `Path.write_text`, `shutil.move`), `utils.load_config`, `utils.read_lock`, `utils.write_lock`, and `subprocess.check_output`.
*   **Details (Example for `test_init_command.py`):**
    ```python
    # tests/unit/test_init_command.py
    import pytest
    from pathlib import Path
    from ai_sdlc.commands import init
    from ai_sdlc import utils

    def test_run_init(temp_project_dir: Path, mocker):
        mocker.patch('ai_sdlc.utils.ROOT', temp_project_dir)
        mock_write_lock = mocker.patch('ai_sdlc.utils.write_lock')

        init.run_init()

        assert (temp_project_dir / "doing").is_dir()
        assert (temp_project_dir / "done").is_dir()
        mock_write_lock.assert_called_once_with({})
        # Could also capture stdout to check print message
    ```

**Task 4.4: Create Integration Tests for CLI (mocking `cursor agent`)**

*   **File:** `tests/integration/test_cli_flow.py` (new)
*   **Actionable:** Use `subprocess.run` to execute `aisdlc` commands. Assert file system state and lock file content. Mock the `cursor agent` call within the `next` command's execution path.
*   **Details:** This is more complex. You might need to:
    1.  Create a helper script that `aisdlc next` can call instead of the real `cursor agent`, which writes predictable output.
    2.  Or, use `pytest-mock` to patch `subprocess.check_output` globally for these tests.
    ```python
    # tests/integration/test_cli_flow.py
    import pytest
    import subprocess
    import json
    from pathlib import Path

    # This assumes 'aisdlc' is installed and in PATH, or you can call it via 'python -m ai_sdlc.cli'
    AISDLC_CMD = ["aisdlc"] # Or ["python", "-m", "ai_sdlc.cli"]

    def run_aisdlc_command(cwd: Path, *args):
        return subprocess.run(
            AISDLC_CMD + list(args),
            capture_output=True,
            text=True,
            cwd=cwd,
            check=False # Handle non-zero exit codes in tests
        )

    @pytest.fixture
    def mock_cursor_agent(mocker):
        def _mock_cursor_agent_call(cmd_args, text, timeout):
            # cmd_args[2] is the path to the temporary prompt file
            # For simplicity, just return a fixed string.
            # A more advanced mock could read the input prompt and return step-specific content.
            if "02-prd-prompt.md" in Path(cmd_args[2]).read_text(): # Rough check
                 return subprocess.CompletedProcess(args=cmd_args, returncode=0, stdout="# Mock PRD Content")
            if "03-prd-plus-prompt.md" in Path(cmd_args[2]).read_text():
                 return subprocess.CompletedProcess(args=cmd_args, returncode=0, stdout="# Mock PRD Plus Content")
            # ... and so on for other steps
            return subprocess.CompletedProcess(args=cmd_args, returncode=0, stdout="# Mock Generic Content")

        return mocker.patch('subprocess.check_output', side_effect=_mock_cursor_agent_call)


    def test_full_lifecycle_flow(temp_project_dir: Path, mock_cursor_agent, mocker):
        # 1. Init
        # Copy minimal .aisdlc and prompt files to temp_project_dir
        # For a real test, you'd copy your actual .aisdlc and prompt templates
        (temp_project_dir / ".aisdlc").write_text('version = "0.1.0"\nsteps = ["01-idea", "02-prd"]\nprompt_dir="prompts"\nactive_dir="doing"\ndone_dir="done"')
        prompts_dir = temp_project_dir / "prompts"
        prompts_dir.mkdir()
        (prompts_dir / "01-idea-prompt.md").write_text("Idea prompt template") # Not used by 'new'
        (prompts_dir / "02-prd-prompt.md").write_text("<prev_step></prev_step>\nPRD Prompt")


        result = run_aisdlc_command(temp_project_dir, "init")
        assert result.returncode == 0
        assert (temp_project_dir / "doing").exists()
        assert (temp_project_dir / "done").exists()
        assert json.loads((temp_project_dir / ".aisdlc.lock").read_text()) == {}

        # 2. New
        idea_title = "My Test Idea"
        idea_slug = "my-test-idea"
        result = run_aisdlc_command(temp_project_dir, "new", idea_title)
        assert result.returncode == 0
        idea_file = temp_project_dir / "doing" / idea_slug / f"01-idea-{idea_slug}.md"
        assert idea_file.exists()
        assert idea_title in idea_file.read_text()
        lock_content = json.loads((temp_project_dir / ".aisdlc.lock").read_text())
        assert lock_content["slug"] == idea_slug
        assert lock_content["current"] == "01-idea"

        # 3. Next (to PRD)
        result = run_aisdlc_command(temp_project_dir, "next")
        assert result.returncode == 0, f"Next command failed. Stderr: {result.stderr}"
        prd_file = temp_project_dir / "doing" / idea_slug / f"02-prd-{idea_slug}.md"
        assert prd_file.exists()
        assert "# Mock PRD Content" in prd_file.read_text() # From mock_cursor_agent
        lock_content = json.loads((temp_project_dir / ".aisdlc.lock").read_text())
        assert lock_content["current"] == "02-prd"

        # (Add more 'next' steps if your mock_cursor_agent and .aisdlc steps support it)

        # 4. Done (assuming all steps are 'completed' by mocks)
        # For this to pass, all 7 files for the '02-prd' step (or last step) would need to exist.
        # This test would need to be more elaborate to simulate all files being created.
        # For now, let's assume '02-prd' is the last step for this simplified test.
        # To make 'done' work, we'd need to manually create the expected files or have the mock 'next' create them.
        # Or, adjust the 'done' command's logic for testing if it's too complex to mock all files.

        # Example: If '02-prd' was the last step in a simplified .aisdlc for this test
        # result = run_aisdlc_command(temp_project_dir, "done")
        # assert result.returncode == 0
        # assert (temp_project_dir / "done" / idea_slug).exists()
        # assert not (temp_project_dir / "doing" / idea_slug).exists()
        # assert json.loads((temp_project_dir / ".aisdlc.lock").read_text()) == {}
    ```

---

### 5. CLI User Experience

**Task 5.1: Add Verbose Output to `aisdlc next`**

*   **File:** `ai_sdlc/commands/next.py`
*   **Actionable:** Add `print` statements to indicate progress.
*   **Details:**
    ```diff
    --- a/ai_sdlc/commands/next.py
    +++ b/ai_sdlc/commands/next.py
    @@ -19,14 +19,20 @@
         print(f"‚ùå  Expected file {prev_file} not found.")
         return
     if not prompt_file.exists():
    -    print(f"‚ùå  Prompt {prompt_file} missing.")
    +    print(f"‚ùå  Prompt file {prompt_file} missing.")
         return

    +print(f"‚ÑπÔ∏è  Reading previous step from: {prev_file}")
    +prev_step_content = prev_file.read_text()
    +print(f"‚ÑπÔ∏è  Reading prompt template from: {prompt_file}")
    +prompt_template_content = prompt_file.read_text()
    +
    -merged_prompt = prompt_file.read_text().replace(PLACEHOLDER, prev_file.read_text())
    +merged_prompt = prompt_template_content.replace(PLACEHOLDER, prev_step_content)
     tmp_prompt_path_str = None # Initialize for finally block
     try:
         with tempfile.NamedTemporaryFile(mode="w+", delete=False, suffix=".md", encoding="utf-8") as tmp_file_obj:
    @@ -34,6 +40,7 @@
             tmp_file_obj.write(merged_prompt)

         print(f"üß†  Calling Cursor agent for step {next_step} ‚Ä¶")
    +    print(f"   Using temporary prompt file: {tmp_prompt_path_str}")
         try:
             output = subprocess.check_output(
                 ["cursor", "agent", "--file", tmp_prompt_path_str],
    @@ -51,6 +58,7 @@
         if tmp_prompt_path_str and Path(tmp_prompt_path_str).exists():
             Path(tmp_prompt_path_str).unlink()

    +print(f"‚ÑπÔ∏è  Cursor agent finished. Writing output to: {next_file}")
     next_file.write_text(output)
     lock["current"] = next_step
     write_lock(lock)

    ```

---

### 6. UV Usage Simplification

**Task 6.1: Review and Simplify UV Workflow in `README.md` and `pyproject.toml`**

*   **Files:** `README.md`, `pyproject.toml`
*   **Actionable:**
    1.  **`pyproject.toml`:**
        *   Ensure `[project.optional-dependencies]` for `dev` is present and includes `pytest`, `pytest-mock`, `ruff`, `pyright`.
        *   The current `[tool.uv]` settings (`virtualenvs.in-project = true`, `sync.subprocesses = true`) are good.
    2.  **`README.md` - Developer Setup:**
        *   Clarify `uv pip install -e .` vs `uv pip install -e .[dev]`.
        *   Emphasize `uv sync` for contributors to install from `uv.lock`.
        *   Ensure commands for linting/testing use `uv run` if appropriate (e.g., `uv run ruff check .`, `uv run pytest`). This ensures tools are run from the virtual environment managed by `uv`.
*   **Details (README.md diff example):**
    ```diff
    --- a/README.md
    +++ b/README.md
    @@ -21,15 +21,20 @@
     | **Node¬†‚â•¬†20¬†/¬†pnpm** *(optional)* | if you plan to extend any TypeScript helpers                | `brew install node pnpm`                                                  |      |

     ```bash
    -# clone & install *editable* for local hacking
    -uv pip install -e .
    -# run the test suite (pytest + ts‚Äëjest if TS code present)
    -pytest -q
+    # Clone the repository
+    # git clone ...
+    # cd ai-sdlc

    -# For consistent dependencies using the lock file (contributors):
-    # uv sync
-    # To update the lock file after changing dependencies in pyproject.toml:
-    # uv lock
+    # Create/activate venv and install dependencies (for contributors, using the lock file):
+    uv venv # Creates .venv if it doesn't exist
+    uv sync # Installs dependencies from uv.lock, including dev dependencies if specified
+
+    # For developers making changes to dependencies in pyproject.toml:
+    # uv pip install -e .[dev] # Installs in editable mode with dev dependencies
+    # uv lock # After changing pyproject.toml, to update uv.lock
+
+    # Run linters and tests (ensure dev dependencies are installed):
+    uv run ruff check ai_sdlc tests
+    uv run pyright
+    uv run pytest -q
     ```

---

================
File: prompts/01-idea-prompt.md
================
You are an expert product strategic thinker. You are contrarion and think in unorthodox ways. You will be given a pitch in the following templated format, 


```` markdown
# Pitch Name

## Problem

## Solution

## Rabbit Holes

````

Your job is to think through the pitch and come up with ways to explore the problem space.

Then you will steelman the solution and come up potential improvements. 

Lastly you will ask if there are any open questions that you think are worth exploring.

Expected output:

```` markdown
# Analysis of [Pitch Name]

## Exploring the Problem Space

[Deeper questions and alternative perspectives on the stated problem. Consider edge cases, underlying causes, and different user segments affected.]

## Steelmanning the Solution & Potential Improvements

[Acknowledge the strengths of the proposed solution. Then, suggest concrete improvements, alternative approaches, or ways to make the solution more impactful, feasible, or innovative. Think about simplifying complexities or adding high-value features.]

## Open Questions Worth Exploring

[Pose specific, actionable questions that would help refine the idea further. These could be about market validation, technical feasibility, user experience, monetization, etc.]
````

---

## Example:

**Input Pitch:**

```markdown
# Pitch Name: AI-Powered Meal Planner

## Problem
Busy professionals and families struggle to consistently plan and prepare healthy meals due to lack of time, inspiration, and knowledge about nutritional balance. This often leads to reliance on unhealthy takeout, food waste from unused groceries, and stress around mealtimes.

## Solution
An AI-powered mobile application that generates personalized weekly meal plans based on user dietary preferences, restrictions, available ingredients (via pantry scanning or manual input), time constraints, and health goals. The app will also generate grocery lists, suggest recipes with step-by-step instructions, and integrate with grocery delivery services.

## Rabbit Holes
- Over-complication of the AI: Trying to make the AI perfectly predict user tastes might be a time sink.
- Data privacy concerns with pantry scanning and health data.
- Scope creep: Adding features like calorie tracking, fitness integration too early.
- Integration complexities with a myriad of grocery delivery services.
```

**Expected LLM Output for the Example:**

```markdown
# Analysis of AI-Powered Meal Planner

## Exploring the Problem Space

While the core problem of meal planning struggles for busy individuals is well-defined, let's delve deeper:

1.  **Beyond "Busy Professionals/Families":** Are there other significant demographics that face this? (e.g., students, elderly individuals living alone, people with specific medical dietary needs). Understanding these nuances could unlock larger or more niche markets.
2.  **Quantifying the "Pain":** How much time *exactly* do people spend on meal planning? What's the average food waste cost per household that this could address? Data here would strengthen the "why now."
3.  **Emotional Component:** The problem mentions "stress." How does this stress manifest? Is it daily anxiety, arguments, or guilt? Understanding the emotional core can inform marketing and feature prioritization.
4.  **Current "Hacks" & Competitors:** What are people *currently* doing? (e.g., Pinterest boards, recipe apps, subscription meal kits, scribbled notes). What are the specific failings of these solutions that our AI planner can uniquely address? Don't just list competitors, analyze their weak points.

## Steelmanning the Solution & Potential Improvements

The proposed AI-powered app is a strong starting point. Let's make it even more robust:

1.  **"Pantry Scanning" - The MVP Approach:** Instead of potentially complex image recognition for pantry scanning initially, could a simpler version involve barcode scanning or integration with past grocery receipts from major retailers (with user permission)? This lowers the barrier to entry and technical complexity.
2.  **"Inspiration" & "Learning" AI:**
    *   **Proactive Suggestions:** Beyond just user input, can the AI learn from commonly paired ingredients, seasonal availability, or even "theme nights" (e.g., "Meatless Monday," "Taco Tuesday") to keep suggestions fresh and reduce decision fatigue?
    *   **Feedback Loop:** How does the AI learn if a user *didn't* like a meal or found a recipe too complex? A simple rating system or "didn't make this" button could be crucial for personalization.
3.  **"Grocery List & Delivery":**
    *   **Smart Aggregation:** If a user has half an onion, does the list add a whole one? Smart handling of partial ingredients is key.
    *   **Prioritized Delivery Partners:** Instead of integrating with *all* services at once, which could be a nightmare, perhaps start with 1-2 major players in key launch markets. Or, provide a generic exportable list initially.
4.  **Beyond Recipes - Skill Building:** Could the app subtly teach cooking skills or nutritional information over time? For example, explaining *why* certain ingredients are paired, or the benefits of a particular cooking method. This adds long-term value beyond just a planning tool.
5.  **Community/Social Angle (Optional - Future):** Could users optionally share successful meal plans (anonymized or with credit) or recipe modifications? This could enhance the "inspiration" aspect.

## Open Questions Worth Exploring

1.  **Monetization Strategy:** Is this a subscription model, a freemium model with premium features (like advanced nutritional analysis or delivery integration), or are there affiliate opportunities with grocery services?
2.  **User Acquisition:** How will the app reach its target audience in a crowded app marketplace?
3.  **Measuring Success:** What are the key metrics for success? Daily active users? Number of meal plans generated? Reduction in self-reported food waste? User retention?
4.  **Scalability of Personalization:** As the user base grows, how will the AI ensure that personalization remains effective and computationally feasible?
5.  **"Time Constraints" Granularity:** How granularly can users specify time? "Under 30 minutes," or specific windows like "only have 15 minutes for prep, 30 for cook"?
6.  **Addressing "Food Moods":** Sometimes people don't want what's planned. How can the app accommodate spontaneous changes or cravings without disrupting the whole week's plan? Perhaps a "quick swap" feature with similar nutritional profiles?

================
File: prompts/02-prd-prompt.md
================
You are an expert technical product manager for feature development.

**Key Responsibilities**

‚Ä¢ **Documentation & Specification:**

Create clear, detailed product requirement documents, including user stories, acceptance criteria, and use cases.

You are a senior product manager and an expert in creating product requirements documents (PRDs) for software development teams.
Your task is to create a comprehensive product requirements document (PRD) for the following project: 

<prev_step>
{this is where the draft of the pitch goes}
</prev_step>

Follow these steps to create the PRD:
‚Äπsteps>
1. Begin with a brief overview explaining the project and the purpose of the document
2. Use sentence case for all headings except for the title of the document, which can be title case, including any you create that are not included in the pra_outline below.
3. Under each main heading include relevant subheadings and fill them with details derived from the prd_instructions
4. Organize your PRD into the sections as shown in the prd_outline below
5. For each section of pro_outline, provide detailed and relevant information based on the PRD instructions. Ensure that you:
   ‚Ä¢ Use clear and concise language
   ‚Ä¢ Provide specific details and metrics where required
   ‚Ä¢ Maintain consistency throughout the document
   ‚Ä¢ Address all points mentioned in each section
6. When creating user stories and acceptance criteria:
- List ALL necessary user stories including primary, alternative, and edge-case scenarios.
- Assign a unique requirement ID (e.g., US-001) to each user story for direct traceability
- Include at least one user story specifically for secure access or authentication if the application requires user identification or access restrictions
- Ensure no potential user interaction is omitted
- Make sure each user story is testable

================
File: prompts/03-prd-plus-prompt.md
================
You are an experienced Senior Product Manager, acting as a "mental jouster." Your primary role is to critically evaluate and rigorously challenge a Product Requirements Document (PRD) provided to you. Your goal is not to be destructive, but to strengthen the PRD by identifying weaknesses, ambiguities, and unexamined assumptions.

When you receive the PRD in the format `<prev_step>{the contents of the prd go here}</prev_step>`, you must:

1.  **Deconstruct Core Objectives:**
    *   Are the primary goals of the product clearly defined?
    *   Are they measurable and achievable?
    *   Do they align with a clear user need or market opportunity?

2.  **Challenge Requirements & Features:**
    *   For each major feature, ask: "Is this essential for the MVP?" "What is the user value vs. development cost?" "What are the dependencies?"
    *   Are the requirements specific, unambiguous, and testable?
    *   Are there any conflicting requirements?
    *   What crucial features might be missing?

3.  **Probe Assumptions:**
    *   What underlying assumptions does this PRD make about users, technology, or the market?
    *   What happens if these assumptions are incorrect?
    *   Are these assumptions validated by data or research?

4.  **Identify Risks & Mitigation:**
    *   What are the key risks (technical, market, resource, etc.) associated with this product?
    *   Does the PRD acknowledge these risks?
    *   Are there any proposed mitigation strategies? Are they adequate?

5.  **Scrutinize User Experience (UX) Considerations:**
    *   Does the PRD adequately address the target user and their journey?
    *   Are there potential UX pain points or unaddressed user needs?
    *   How will accessibility be handled?

6.  **Examine Success Metrics:**
    *   Are the Key Performance Indicators (KPIs) or success metrics clearly defined?
    *   Are they directly tied to the product's objectives?
    *   How will these metrics be tracked and reported?
    *   Are there any vanity metrics that could be misleading?

7.  **Consider Edge Cases & Scalability:**
    *   What are the potential edge cases or failure scenarios? How will the product handle them?
    *   Has scalability been considered in the requirements?
    *   Are there any potential negative consequences or misuse scenarios?

8.  **Question Prioritization:**
    *   Is the prioritization of features logical and justified?
    *   Are there any low-value features that could be deferred or cut?

Your output should be a series of pointed questions, challenges, and potential concerns, framed constructively to help the original author refine and improve the PRD. Think like a devil's advocate who ultimately wants the product to succeed.

================
File: prompts/04-architecture-prompt.md
================
--- 
title: "AI Software Architect: Project Architecture Generation"
description: "This prompt guides an AI to act as a Senior Software Architect, analyzing a project's PRD-plus and current file structure to propose an optimal, simple, and quickly implementable architecture."
version: 1.0
author: AI Assistant
---

You are a Senior Software Architect. Your task is to analyze the provided **Product Requirements Document (PRD-plus)** and the current project's file structure to design and recommend an improved software architecture. Prioritize architectures that are **simple, robust, and can be implemented efficiently.**

**Input:**

You will receive the PRD-plus for the project in the following block:
```
<prev_step>
{the contents of the prd-plus go here}
</prev_step>
```

**Instructions for the Architect (You):**

1.  **Understand the Project:** Thoroughly review the PRD-plus to grasp the project's goals, features, constraints, and scope.
2.  **Analyze Current File Structure:**
    *   You need to visualize the current file structure of the project.
    *   **If you have terminal access and are on a macOS system (or a system where `tree` is available):**
        Assume the user will run a command like:
        `tree -L 4 -I '.git|node_modules|*env*|__pycache__|.DS_Store|*.egg-info|dist|build|target'`
        (Note: The user might need to install `tree` via Homebrew: `brew install tree`)
    *   **If direct file tree generation is not possible through your environment:**
        The user should provide the current file tree structure as text. If a file tree is not explicitly provided *after* the PRD-plus, you should state that you need this information or proceed based on any contextual clues if available.
    *   The prompt you are currently processing *may* include an example "Current File Structure" in the output template below. Use that as a placeholder if no live data is provided.

3.  **Propose a New File Structure:**
    *   Based on the PRD-plus and the current structure (if available), design an improved file structure.
    *   **Guiding Principle:** Favor simplicity and speed of implementation. Choose patterns and structures that are well-understood and reduce cognitive overhead. Avoid over-engineering.
    *   Clearly present the proposed file tree.

4.  **Explain Your Architectural Choices:**
    *   Provide a clear rationale for the proposed architecture.
    *   If a "Current File Structure" was available, compare your proposal to it, outlining key issues with the current one (if any) and the benefits of your new structure.
    *   Highlight how it aligns with the project requirements and the principles of simplicity and robustness.

5.  **Detail System Patterns:**
    Elaborate on the following aspects of your proposed architecture:
    *   **A. System Architecture:** (e.g., Modular Monolith, Layered Architecture, Microservices (if truly justified by scale and complexity, but prefer simpler alternatives first), Event-Driven, etc.). Explain your choice.
    *   **B. Key Technical Decisions:**
        *   Core programming languages and frameworks.
        *   Database choices (if applicable).
        *   Key libraries or tools that are central to the architecture.
        *   Rationale for these decisions.
    *   **C. Design Patterns in Use:**
        *   Identify 2-3 key design patterns that will be central to the proposed architecture.
        *   For each pattern, provide a brief description and explain its relevance and application in this project, considering the primary language(s) (Python, JavaScript, TypeScript). (Refer to the common patterns list below if needed).
    *   **D. Component Relationships:**
        *   Describe how the major components/modules in your proposed structure will interact.
        *   A simple diagram concept (text-based) or a clear textual description is sufficient.
    *   **E. Critical Implementation Paths:**
        *   Outline the key steps or sequence of development to implement the core of the proposed architecture. This helps in planning and prioritizing.

**Output Format:**

Present your response in a clear, structured Markdown format as follows:

---

## 1. Current File Structure

*(This section is for the actual current file structure. If obtained by running a command like `tree -L 4 -I '.git|node_modules|*env*|__pycache__|.DS_Store|*.egg-info|dist|build|target'`, paste the output here. If provided by the user or inferred, place it here. If not available, state so.)*

```
# Example placeholder:
# .
# ‚îú‚îÄ‚îÄ src/
# ‚îÇ   ‚îî‚îÄ‚îÄ main.py
# ‚îú‚îÄ‚îÄ tests/
# ‚îÇ   ‚îî‚îÄ‚îÄ test_main.py
# ‚îî‚îÄ‚îÄ README.md
```

## 2. Proposed File Structure

```
# Your proposed file tree goes here.
# Example placeholder:
# .
# ‚îú‚îÄ‚îÄ my_project_name/
# ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
# ‚îÇ   ‚îú‚îÄ‚îÄ app.py
# ‚îÇ   ‚îú‚îÄ‚îÄ core/
# ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
# ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services.py
# ‚îÇ   ‚îú‚îÄ‚îÄ models/
# ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
# ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ user.py
# ‚îÇ   ‚îî‚îÄ‚îÄ api/
# ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
# ‚îÇ       ‚îî‚îÄ‚îÄ routes.py
# ‚îú‚îÄ‚îÄ tests/
# ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
# ‚îÇ   ‚îî‚îÄ‚îÄ test_app.py
# ‚îú‚îÄ‚îÄ .gitignore
# ‚îú‚îÄ‚îÄ pyproject.toml
# ‚îî‚îÄ‚îÄ README.md
```

## 3. Architectural Explanation

*(Your detailed explanation of the proposed architecture.
- If a "Current File Structure" was available, compare your proposal to it, outlining key issues with the current one and the benefits of your new structure.
- Explain why your proposed architecture is suitable for the project based on the PRD-plus.
- Emphasize how it aligns with principles of simplicity, robustness, and rapid implementation.)*

## 4. System Patterns

### A. System Architecture

*(Describe the overall architectural style chosen, e.g., Layered, Modular Monolith, etc., and justify it.)*

### B. Key Technical Decisions

*(List and justify choices for languages, frameworks, databases, key libraries.)*

### C. Design Patterns in Use

*(Identify key design patterns relevant to YOUR proposed architecture. For each:
1. Briefly describe the pattern.
2. Explain its specific application and benefit within this project's context and chosen language(s).)*

---
**Reference: Common Design Patterns**

*You can use this list as a reference. Focus on patterns most applicable to the project.*

*   **General Patterns:**
    *   **Singleton:** Ensures a class has only one instance and provides a global point of access to it. *Use sparingly, as it can introduce global state and make testing harder.*
    *   **Factory (Abstract Factory, Factory Method):** Creates objects without exposing the instantiation logic to the client, promoting loose coupling.
    *   **Facade:** Provides a simplified interface to a larger body of code, such as a complex subsystem, making it easier to use.
    *   **Observer (Publish/Subscribe):** Defines a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically. Useful for event-driven systems.
    *   **Strategy:** Defines a family of algorithms, encapsulates each one, and makes them interchangeable. Strategy lets the algorithm vary independently from clients that use it.
    *   **Decorator:** Allows behavior to be added to an individual object, dynamically, without affecting the behavior of other objects from the same class.
    *   **Repository:** Mediates between the domain and data mapping layers using a collection-like interface for accessing domain objects. Abstracts data persistence.
    *   **Dependency Injection (DI):** A technique whereby one object supplies the dependencies of another object. Promotes loose coupling and testability.

*   **Python Specific:**
    *   **Context Managers (`with` statement):** Manage resources, ensuring they are set up and torn down correctly (e.g., file handling, database connections, locks).
    *   *Many general patterns like Factory, Decorator, Observer, Strategy, Repository are directly applicable and widely used in Python.*

*   **JavaScript/TypeScript Specific:**
    *   **Module Pattern (IIFE, ES6 Modules):** Encapsulates a collection of related methods and variables into a single unit, often using closures or ES6 module syntax to create private/public members.
    *   **Constructor/Class Pattern:** Used for creating objects (ES6 classes are syntactic sugar over constructor functions and prototypes).
    *   **Middleware (e.g., in Express.js, Koa.js):** Functions that have access to the request object, the response object, and the next middleware function in the application‚Äôs request-response cycle. Powerful for handling cross-cutting concerns.
    *   **Provider Pattern (e.g., React Context API):** A way to pass data through the component tree without having to pass props down manually at every level.
    *   **Higher-Order Components (HOCs - e.g., React):** Functions that take a component and return a new component, often used for reusing component logic. (Render Props and Hooks are often preferred alternatives now).
    *   **Async/Await for Promises:** While not a design pattern in the traditional sense, mastering its use is crucial for handling asynchronous operations cleanly.
    *   *Many general patterns like Factory, Singleton, Observer, Facade, Repository are directly applicable.*
---

*(Now, detail the specific patterns chosen for THIS project and their application, referring back to the section "C. Design Patterns in Use" above.)*

### D. Component Relationships

*(Describe how the major components/modules in your proposed structure interact. This can be text or a simple diagram conceptualization, e.g., a list of components and their primary collaborators.)*

### E. Critical Implementation Paths

*(Outline the key steps or sequence of development to implement the core functionality based on the proposed architecture. For example:
1. Setup basic project structure and CI/CD.
2. Implement core domain models and business logic.
3. Develop API endpoints for key features.
4. Integrate with essential external services.
5. Write unit and integration tests for core paths.)*

---

**Final Note for the AI:**
Remember to tailor your architectural recommendations to the specifics of the PRD-plus. The goal is a practical, effective, and streamlined architecture. Avoid unnecessary complexity unless explicitly justified by the project's requirements.

================
File: prompts/05-systems-patterns.md
================
---
title: "System Patterns Documentation Generation"
description: "This prompt guides an AI to take architectural decisions from a previous step and generate or update the 'rules/system.md' file, which serves as the canonical documentation for the system's architecture and patterns."
version: 1.0
author: AI Assistant
---

You are a Senior Technical Writer and System Architect. Your task is to use the architectural decisions and system pattern information provided from the previous step (the output of `04-architecture-prompt.md`) to generate or update the `rules/system.md` file. This file is the **single source of truth** for the project's architecture, design patterns, and technical guidelines.

**Input:**

You will receive the detailed architectural plan (which was the output of `04-architecture-prompt.md`) in the following block:

```
<prev_step>
{Output from 04-architecture-prompt.md containing the proposed architecture, file structures, architectural explanation, and system pattern details goes here}
</prev_step>
```

**Your Primary Goal:**

To produce the complete content for the `rules/system.md` file. This file should be comprehensive, clear, and actionable for the development team.

**Instructions for Generating/Updating `rules/system.md`:**

1.  **Understand the New Architecture:** Thoroughly analyze the architectural plan provided in `<prev_step>`. Pay close attention to:
    *   The overall chosen system architecture (e.g., Layered, Modular Monolith, Microservices if any).
    *   The proposed file structures for frontend and backend.
    *   Key technical decisions (languages, frameworks, databases).
    *   Specific design patterns selected for use and their justifications.
    *   Component relationships and critical implementation paths.

2.  **Handle `rules/system.md` Content:**
    *   **If `rules/system.md` is being created (or is conceptually empty):** Generate all necessary sections for `rules/system.md` from scratch based on the input. The structure provided below should be your guide.
    *   **If `rules/system.md` conceptually has existing content (imagine you are updating it):**
        *   Your output should be the *complete, updated content* for `rules/system.md`.
        *   **Identify** sections in the (conceptual) existing `rules/system.md` that correspond to the architectural elements in the new input from `<prev_step>` (e.g., "Overall Architecture," "Backend Patterns," "Frontend Patterns").
        *   **Update these sections thoroughly** with the latest decisions. Ensure the information is consistent with the new architectural plan.
        *   **Add new sections** if the new architecture introduces concepts not previously documented (e.g., a new cross-cutting concern, a specific new pattern).
        *   **Remove or clearly reframe/deprecate** any information in the (conceptual) existing `rules/system.md` that is directly contradicted or made obsolete by the new architectural decisions. Ambiguities should be resolved in favor of the new plan.
        *   The final output for `rules/system.md` must be a cohesive document reflecting the **current and authoritative** architectural plan.

3.  **Structure for `rules/system.md`:**
    Organize the `rules/system.md` file with the following sections. Use Markdown for formatting. Include Mermaid diagrams where they aid understanding (especially for overall architecture and component relationships), as specified in the input from `04-architecture-prompt.md`.

    ```markdown
    # System Architecture & Design Patterns (`rules/system.md`)

    **This document is the canonical source for the project's system architecture, design patterns, and technical guidelines. All development work must align with the principles and structures outlined herein. Deviations require explicit approval and documentation.**

    ## 1. Overall Architecture Philosophy

    *   (Describe the overarching architectural approach, e.g., distributed system, monolithic, modular monolith. Explain the core philosophy guiding the design, such as separation of concerns, scalability, maintainability, etc., as derived from the `<prev_step>` input.)
    *   (Include a high-level Mermaid diagram illustrating the main components and their interactions, if provided in the input.)
    
    ```mermaid
    graph TD
        UserExample[User] -- HTTPS --> FrontendExample[Frontend]
        FrontendExample -- API Calls --> BackendExample[Backend API]
        BackendExample -- Interacts with --> DatabaseExample[Database]
        BackendExample -- Interacts with --> ExternalServicesExample[External Services]
    ```
    *(Replace the above diagram with the actual one if available from input)*

    ## 2. Backend System Patterns

    ### 2.1. Core Backend Architecture
    *   (Detail the chosen backend architecture, e.g., Clean Architecture, Layered, etc., as specified in `<prev_step>`. Explain its layers and principles.)
    *   (Include a Mermaid diagram illustrating the backend layers and their dependencies, if provided.)

    ### 2.2. Key Backend Design Patterns
    *   (List and describe each key design pattern chosen for the backend, as detailed in `<prev_step>`. For each pattern, explain its purpose and how it will be applied in the backend codebase. Example: Repository Pattern, Factory Pattern, Dependency Injection, etc.)

    ### 2.3. Backend Component Relationships & Flows
    *   (Describe how major backend components/modules interact. Detail key data flows or request-response cycles, as per `<prev_step>`.)
    *   (A Mermaid sequence or activity diagram might be useful here if data is available in input.)

    ### 2.4. Backend Module/Directory Structure
    *   (Present the proposed backend directory structure from `<prev_step>` and briefly explain the purpose of key directories.)

    ## 3. Frontend System Patterns (If Applicable)

    *(If the project includes a significant frontend, detail its patterns. If not, this section can be omitted or state that it's not applicable.)*

    ### 3.1. Core Frontend Architecture
    *   (Detail the chosen frontend architecture, e.g., Component-Based SPA, MVC, MVVM, as specified in `<prev_step>`.)
    *   (Include a Mermaid diagram illustrating frontend architecture if provided.)

    ### 3.2. Key Frontend Technical Decisions & Patterns
    *   (List and describe key frontend patterns and technical decisions, e.g., State Management approach, Routing strategy, Component Design, API communication, as per `<prev_step>`.)

    ### 3.3. Frontend Component Relationships & Structure
    *   (Describe interactions between major frontend components/modules.)
    *   (Present the proposed frontend directory structure from `<prev_step>` and explain key directories.)

    ### 3.4. Critical Frontend Implementation Paths/Flows
    *   (Outline key user flows or implementation paths for the frontend, as described in `<prev_step>`.)

    ## 4. Cross-Cutting Concerns & Platform-Wide Patterns

    *   **(Detail platform-wide patterns and approaches for concerns like:**
        *   **Error Handling:** (Strategy for backend and frontend.)
        *   **Logging:** (Approach for backend and frontend.)
        *   **Validation:** (Data validation strategies.)
        *   **Security:** (Key security measures, authentication/authorization mechanisms.)
        *   **Configuration Management:** (How configuration is handled.)
        *   **API Design Principles:** (RESTful conventions, versioning, etc., if not covered elsewhere.)
        *   **Testing Strategy:** (Overall approach to unit, integration, E2E tests.)
        *   *(Add other relevant cross-cutting concerns from `<prev_step>`)*

    ## 5. Key Technology Stack Summary

    *   **(List the primary technologies, languages, frameworks, and significant libraries decided in `<prev_step>` for both backend and frontend, e.g.:**
        *   **Backend:** Python 3.x, FastAPI, PostgreSQL, SQLAlchemy, Pydantic, etc.
        *   **Frontend:** TypeScript, React, Next.js, Zustand, TanStack Query, etc.
        *   **Common Tools:** Docker, Git, etc.)

    --- 
    *This document should be reviewed and updated as the system evolves.*
    ```

4.  **Clarity and Actionability:** Ensure the generated content for `rules/system.md` is written in clear, unambiguous language. The rules and guidelines should be practical and directly usable by developers.

**Final Output:**

Your entire response should be the complete, final content for the `rules/system.md` file, ready to be saved.

================
File: prompts/06-tasks-prompt.md
================
---
# AI-Assisted Project Execution Plan & Task List Template

**Document Purpose:** This document serves as both an **exemplar project plan** and a **template** for generating a new, comprehensive, step-by-step task list. It is designed to be used by an AI assistant within a prompt chain.

**Intended Audience & Use:**
*   **Development Team:** As a model for well-defined project plans and task specifications.
*   **AI Assistant (You):** As a detailed structural guide and stylistic example. Your primary goal when using this template is to take an input Product Requirements Document (PRD) (likely provided from a previous step in a prompt chain, e.g., via a variable like `<prd_content>`) and generate a *new* set of atomic, detailed tasks for *that specific PRD*, mirroring the format, level of detail, and checkbox style demonstrated herein.

**This Document as an Exemplar:** The tasks outlined below for the "Video Processor Refactoring" project are a concrete example of the desired output. You should generate a *new and different* set of tasks relevant to the PRD you are given, but following this structural and stylistic pattern.

**Critical Contextual Documents (for the input PRD):**
When processing a new PRD, you will need to refer to it and any associated architectural or system pattern documents that might be provided alongside it to generate a relevant and accurate task list.
For the *exemplar* tasks within *this* document, the following were used (and you would expect similar for a new PRD):
*   `pitch.md` (Exemplar context)
*   `prd.md` (Exemplar context)
*   `architecture.md` (Exemplar context)
*   `system-patterns.md` (Exemplar context)

**Instructions for AI Assistant (You) when Generating a *New* Task List:**
1.  **Input PRD Focus:** Your generated task list must be based *entirely* on the new PRD provided to you.
2.  **Follow the Format:** Replicate the hierarchical structure (Phases, Task Groups, Tasks), the use of "Objective," "Action(s)," and "Verification/Deliverable(s)" for each task, and the markdown formatting.
3.  **Task Generation & Checkboxes:** Generate new tasks with `[ ]` (empty checkbox) as their initial status. The `[x]` in the exemplar below indicates a *completed* example task.
4.  **Atomicity:** Break down the work from the PRD into the smallest logical, actionable, and verifiable steps.
5.  **Detail:** Provide as much specific detail as possible for each task, including file paths, code snippets, or configuration examples if appropriate and inferable from the PRD or associated documents.

---

## Project Plan: Video Processor Refactoring

**Overall Project Goal:** Refactor the existing video processing logic from the legacy `video_processor` directory into the new `apps/core` application architecture. This refactor will leverage Supabase for Authentication and PostgreSQL database services. A key objective is to establish a robust, modern backend with excellent developer experience, including local development parity with production.

---

### **Phase 0: Project Setup & Essential Configuration**

#### **1. Directory Structure & Dependency Setup**

*   **Task 0.1: Verify/Create Core Application Directories in `apps/core/`** [x]
    *   **Objective:** Establish the foundational directory structure for the `apps/core` module.
    *   **Action(s):** Ensure the following directories exist within `apps/core/`. If a directory is missing, create it.
        *   `apps/core/lib/ai/`
        *   `apps/core/lib/auth/`
        *   `apps/core/lib/database/`
        *   `apps/core/lib/messaging/` (if not already present from a previous setup)
        *   `apps/core/lib/publishing/`
        *   `apps/core/lib/storage/`
        *   `apps/core/lib/utils/`
        *   `apps/core/api/schemas/`
        *   `apps/core/api/endpoints/`
        *   `apps/core/models/`
        *   `apps/core/operations/`
        *   `apps/core/services/`
        *   `apps/core/core/` (for shared core logic like exceptions, configuration)
        *   `apps/core/tests/unit/`
        *   `apps/core/tests/integration/`
    *   **Verification/Deliverable(s):** A correctly structured `apps/core/` directory containing all specified subdirectories.

*   **Task 0.2: Initialize Python Dependencies in `apps/core/pyproject.toml`** [x]
    *   **Objective:** Define and prepare the necessary Python libraries for the `apps/core` application.
    *   **Action(s):**
        1.  Open `apps/core/pyproject.toml`.
        2.  Add the following core dependencies:
            *   `fastapi`, `uvicorn`
            *   `sqlalchemy` (for ORM)
            *   `psycopg2-binary` (PostgreSQL adapter)
            *   `pydantic` (with email validation, settings management)
            *   `pydantic-settings` (for loading from .env)
            *   `python-jose[cryptography]` (for JWT handling)
            *   `python-multipart` (for file uploads)
            *   `google-cloud-storage` (if Google Cloud Storage is planned for use)
            *   Relevant AI SDKs (e.g., `google-generativeai`, `google-cloud-aiplatform`)
            *   `alembic` (for database migrations)
            *   `supabase` (Python client, if direct interaction beyond auth is needed)
        3.  Add testing libraries:
            *   `pytest`, `pytest-cov`, `httpx` (for FastAPI TestClient)
        4.  Ensure your project uses a dependency manager like `uv`, `poetry`, or `pdm` to install and manage these dependencies based on `pyproject.toml`.
            *   Example installation command (using `uv`): `uv pip install fastapi uvicorn sqlalchemy psycopg2-binary pydantic pydantic-settings "python-jose[cryptography]" python-multipart google-cloud-storage google-generativeai alembic supabase pytest pytest-cov httpx`
    *   **Verification/Deliverable(s):** Updated `apps/core/pyproject.toml` with all specified dependencies. Dependencies potentially installed in the project's virtual environment.

#### **2. Supabase Local & Cloud Setup**

*   **Task 0.2.1: Set Up Cloud Supabase Project** [x]
    *   **Objective:** Create and configure the cloud-hosted Supabase project.
    *   **Action(s):**
        1.  Go to [supabase.com](https://supabase.com) and create a new project.
        2.  In the project settings, navigate to the Auth providers section and enable Google Auth.
    *   **Verification/Deliverable(s):** A Supabase project available in the cloud with Google Auth enabled.

*   **Task 0.2.2: Obtain Cloud Supabase Project Credentials** [x]
    *   **Objective:** Securely record the necessary credentials from the cloud Supabase project.
    *   **Action(s):**
        1.  Navigate to your cloud Supabase project settings (Project Settings > API).
        2.  Carefully note down the following:
            *   Project URL
            *   Anon key (public)
            *   Service role key (secret - **handle with extreme care**)
            *   JWT Secret (found under Project Settings > API > JWT Settings)
    *   **Verification/Deliverable(s):** A record of the Project URL, Anon key, Service role key, and JWT Secret for the cloud Supabase project.

*   **Task 0.2.3: Initialize Local Supabase Environment using Supabase CLI** [x]
    *   **Objective:** Set up the local Supabase development environment for local development and testing.
    *   **Action(s):**
        1.  **Install Supabase CLI:** If not already installed, execute `npm install supabase --save-dev` in your project's root, or install globally as per your preference.
            *   *Verification:* Confirm the `supabase` command is accessible in your terminal.
        2.  **Initialize Supabase Project:**
            *   Navigate to your workspace root (or create a dedicated `supabase/` directory if you prefer to manage Supabase configuration files there).
            *   Run the command: `supabase init`.
            *   *Expected Result:* A `supabase` configuration folder is created in the current directory.
        3.  **Start Local Supabase Services:**
            *   Run the command: `supabase start`.
            *   *Expected Result:* Docker images are downloaded (if it's the first time), and local Supabase services (API, Database, Auth, etc.) are started.
            *   **Crucial:** Carefully note the following details provided in the command output:
                *   Local API URL (e.g., `http://localhost:54321`)
                *   Local Anon key (public)
                *   Local Service role key (secret)
                *   Local Default JWT secret
                *   Local Database URL (e.g., `postgresql://postgres:postgres@localhost:54322/postgres`)
    *   **Verification/Deliverable(s):** A running local Supabase instance. All local Supabase credentials (URL, keys, JWT secret, DB URL) noted down.

*   **Task 0.2.4: Configure Environment Variables (`.env`) for `apps/core`** [x]
    *   **Objective:** Set up environment variables for local development, including Supabase credentials and other service configurations.
    *   **Action(s):**
        1.  Create an example environment file `apps/core/.env.example` with placeholders for all required variables.
        2.  Create the actual environment file `apps/core/.env` (this file **must** be added to `.gitignore` to prevent committing secrets).
        3.  Populate `apps/core/.env` with your *local Supabase* credentials (obtained from `supabase start` output in Task 0.2.3) and any development API keys for services like AI (e.g., Gemini) or Google Cloud Storage.
    *   **Content for `apps/core/.env.example` (and to be filled in `.env`):**
        ```dotenv
        # apps/core/.env.example
        ENVIRONMENT="development" # Options: "development", "production", "test"

        # Supabase (Populate with Local Dev Defaults from 'supabase start' output)
        SUPABASE_URL="http://localhost:54321"
        SUPABASE_ANON_KEY="your_local_anon_key"
        SUPABASE_SERVICE_ROLE_KEY="your_local_service_role_key" # For backend use
        SUPABASE_JWT_SECRET="your_local_default_jwt_secret_at_least_32_characters_long" # Ensure this is strong
        DATABASE_URL="postgresql://postgres:postgres@localhost:54322/postgres" # Default local Supabase DB URL

        # AI Service (Example: Gemini)
        GEMINI_API_KEY="your_development_gemini_api_key"
        # OPENAI_API_KEY="your_development_openai_api_key" # If using OpenAI

        # Storage Configuration
        STORAGE_BACKEND="local" # Options: "local", "gcs" (for Google Cloud Storage in production)
        LOCAL_STORAGE_PATH="./output_files" # Directory for local file storage (relative to apps/core)
        # GCS_BUCKET_NAME="your_gcs_bucket_name" # Required for 'gcs' backend
        # GOOGLE_APPLICATION_CREDENTIALS_PATH="/path/to/your/gcs_service_account.json" # Required for 'gcs' backend

        # Redis (Optional, if used for caching or other purposes)
        REDIS_HOST="localhost"
        REDIS_PORT="6379"
        REDIS_DB="0"
        REDIS_PASSWORD="" # Set if your Redis requires a password

        # General API Settings
        PROJECT_NAME="AI-Driven Backend Service"
        API_PREFIX="/api/v1"
        DEBUG="True" # Set to False in production

        # JWT Settings (for application-specific tokens if any, distinct from Supabase JWT)
        # SECRET_KEY="a_very_secret_key_for_your_app_specific_jwt" # Replace with a strong, random key
        # ALGORITHM="HS256"
        # ACCESS_TOKEN_EXPIRE_MINUTES="30"

        # Email Settings (If sending emails directly from the app)
        # SMTP_SERVER="smtp.example.com"
        # SMTP_PORT="587"
        # SMTP_USERNAME="your_email_username"
        # SMTP_PASSWORD="your_email_password"
        # EMAIL_FROM_ADDRESS="noreply@example.com"
        # EMAIL_TEMPLATES_DIR="templates/emails" # Relative to apps/core

        # File Upload Configuration
        UPLOAD_DIR="uploads" # Relative to apps/core, for temporary storage or direct uploads
        ```
    *   **Verification/Deliverable(s):**
        *   `apps/core/.env.example` created and populated with placeholders.
        *   `apps/core/.env` created, populated with local development credentials, and added to `.gitignore`.

*   **Task 0.2.5: Implement Configuration Loading in `apps/core/core/config.py`** [x]
    *   **Objective:** Create a robust mechanism for loading application settings from environment variables using Pydantic.
    *   **Action(s):** Create or update the file `apps/core/core/config.py` to define a Pydantic `Settings` class. This class should inherit from `pydantic_settings.BaseSettings` and load configurations from the `.env` file.
    *   **Code for `apps/core/core/config.py`:**
        ```python
        # apps/core/core/config.py
        from typing import Optional, Literal
        from pydantic_settings import BaseSettings
        from pydantic import PostgresDsn, RedisDsn
        from pathlib import Path

        class Settings(BaseSettings):
            ENVIRONMENT: Literal["development", "production", "test"] = "development"

            # Supabase
            SUPABASE_URL: str
            SUPABASE_ANON_KEY: str
            SUPABASE_SERVICE_ROLE_KEY: Optional[str] = None  # Secret, use with care
            SUPABASE_JWT_SECRET: str # For validating Supabase JWTs
            DATABASE_URL: PostgresDsn # Pydantic will validate this as a PostgreSQL DSN

            # AI Services
            GEMINI_API_KEY: Optional[str] = None
            OPENAI_API_KEY: Optional[str] = None

            # Storage
            STORAGE_BACKEND: Literal["local", "gcs"] = "local"
            LOCAL_STORAGE_PATH: str = "output_files" # Relative path from apps/core
            GCS_BUCKET_NAME: Optional[str] = None
            GOOGLE_APPLICATION_CREDENTIALS_PATH: Optional[Path] = None

            # Redis
            REDIS_HOST: str = "localhost"
            REDIS_PORT: int = 6379
            REDIS_DB: int = 0
            REDIS_PASSWORD: Optional[str] = None
            # REDIS_URL: Optional[RedisDsn] = None # Alternative: provide full Redis DSN

            # API settings
            PROJECT_NAME: str = "AI-Driven Backend Service"
            API_PREFIX: str = "/api/v1"
            DEBUG: bool = False

            # Application-specific JWT settings (if needed, distinct from Supabase)
            # SECRET_KEY: str = "your_app_secret_key_change_me" # For signing app-specific tokens
            # ALGORITHM: str = "HS256"
            # ACCESS_TOKEN_EXPIRE_MINUTES: int = 30

            # Email settings
            # SMTP_SERVER: Optional[str] = None
            # SMTP_PORT: Optional[int] = None
            # SMTP_USERNAME: Optional[str] = None
            # SMTP_PASSWORD: Optional[str] = None
            # EMAIL_FROM_ADDRESS: Optional[str] = None
            # EMAIL_TEMPLATES_DIR: str = "templates/emails"

            # File Uploads
            UPLOAD_DIR: str = "uploads" # Relative path from apps/core

            # Base Directory of the 'apps/core' application
            BASE_DIR: Path = Path(__file__).resolve().parent.parent

            class Config:
                env_file = ".env"
                env_file_encoding = "utf-8"
                extra = "ignore"  # Ignore extra fields from .env not defined in Settings

        settings = Settings()

        # Ensure paths are resolved correctly relative to BASE_DIR
        settings.LOCAL_STORAGE_PATH = str(settings.BASE_DIR / settings.LOCAL_STORAGE_PATH)
        settings.UPLOAD_DIR = str(settings.BASE_DIR / settings.UPLOAD_DIR)
        # if settings.EMAIL_TEMPLATES_DIR:
        #     settings.EMAIL_TEMPLATES_DIR = str(settings.BASE_DIR / settings.EMAIL_TEMPLATES_DIR)
        ```
    *   **Verification/Deliverable(s):**
        *   `apps/core/core/config.py` created and populated.
        *   The `settings` object can be imported and successfully loads values from `apps/core/.env`.

*   **Task 0.2.6: Confirm Database Connection Setup (`apps/core/lib/database/connection.py`)** [x]
    *   **Objective:** Ensure the database connection module is correctly set up for SQLAlchemy and FastAPI integration.
    *   **Action(s):** Review `apps/core/lib/database/connection.py`. Confirm it provides:
        1.  A SQLAlchemy session generator compatible with FastAPI dependency injection (e.g., `get_db_session`).
        2.  The SQLAlchemy declarative base (`Base = declarative_base()`).
        3.  Uses `settings.DATABASE_URL` from `apps.core.core.config` to create the SQLAlchemy engine.
    *   **Note:** The prompt indicates this file is likely already correctly configured. This task is primarily a verification step.
    *   **Verification/Deliverable(s):** `apps/core/lib/database/connection.py` is confirmed to meet requirements for SQLAlchemy integration with FastAPI and Alembic.

*   **Task 0.2.7: Initialize and Configure Database Migrations (Alembic)** [x]
    *   **Objective:** Set up Alembic for managing database schema migrations within the `apps/core` application.
    *   **Action(s):**
        1.  **Initialize Alembic:** If not already done, navigate to the `apps/core/` directory and run: `alembic init alembic`. This creates an `alembic` directory and an `alembic.ini` file.
        2.  **Configure `apps/core/alembic/env.py`:**
            *   Modify `env.py` to import `Base` from `apps.core.lib.database.connection` (or wherever your SQLAlchemy declarative base is defined). Set `target_metadata = Base.metadata`.
            *   Import `settings` from `apps.core.core.config`.
            *   Configure the database connection URL for Alembic. Ensure the `run_migrations_online()` function uses `settings.DATABASE_URL`. One common way is to set `config.set_main_option('sqlalchemy.url', str(settings.DATABASE_URL))`.
        3.  **Configure `apps/core/alembic.ini`:**
            *   Locate the `sqlalchemy.url` setting. It's best practice to comment this out or leave it blank in `alembic.ini` and explain in a comment that the URL is set dynamically from `settings.DATABASE_URL` in `env.py` to avoid hardcoding connection strings.
        4.  **Initial Migration (Post-Model Definition):** After defining initial SQLAlchemy models (in Phase 1), you will create the first migration.
            *   Command (from `apps/core/` directory): `alembic revision -m "initial_setup_video_processing_tables"`
            *   Then apply it: `alembic upgrade head`
            *   If specific guidelines like `sb-create-migration` (Supabase CLI migration helper) are relevant for local Supabase development, ensure adherence.
    *   **Verification/Deliverable(s):**
        *   Alembic initialized in `apps/core/`.
        *   `env.py` and `alembic.ini` correctly configured to use the application's database settings and metadata.
        *   (LATER, after Phase 1 models) An initial migration can be successfully generated and applied.

---

### **Phase 1: Foundation - Models and Core Libraries (Libs)**

#### **3. Define Core Data Models (`apps/core/models/`)**
    *   **General Requirement:** All ORM models must inherit from `Base` (the declarative base) defined in `apps.core.lib.database.connection`.

*   **Task 3.1: Define `VideoModel` in `apps/core/models/video_model.py`** [ ]
    *   **Objective:** Create the SQLAlchemy model for storing video metadata.
    *   **Action(s):** Define the `VideoModel` class inheriting from `Base`.
    *   **Attributes:**
        *   `id`: `Column(Integer, primary_key=True, autoincrement=True)`
        *   `uploader_user_id`: `Column(String, index=True, nullable=False)` (Corresponds to Supabase Auth user ID)
        *   `original_filename`: `Column(String, nullable=False)`
        *   `storage_path`: `Column(String, unique=True, nullable=False)` (Path in GCS or local filesystem)
        *   `content_type`: `Column(String, nullable=False)`
        *   `size_bytes`: `Column(Integer, nullable=False)`
        *   `created_at`: `Column(DateTime, default=func.now(), nullable=False)`
        *   `updated_at`: `Column(DateTime, default=func.now(), onupdate=func.now(), nullable=False)`
    *   **Key File(s):** `apps/core/models/video_model.py`
    *   **Verification/Deliverable(s):** `VideoModel` class correctly defined.

*   **Task 3.2: Define `ProcessingStatus` Enum in `apps/core/models/enums.py`** [ ]
    *   **Objective:** Create an enumeration for video job processing statuses.
    *   **Action(s):** Define the `ProcessingStatus` enum.
    *   **Definition:**
        ```python
        # apps/core/models/enums.py
        import enum

        class ProcessingStatus(str, enum.Enum):
            PENDING = "PENDING"
            PROCESSING = "PROCESSING"
            COMPLETED = "COMPLETED"
            FAILED = "FAILED"
        ```
    *   **Key File(s):** `apps/core/models/enums.py`
    *   **Verification/Deliverable(s):** `ProcessingStatus` enum correctly defined.

*   **Task 3.3: Define `VideoJobModel` in `apps/core/models/video_job_model.py`** [ ]
    *   **Objective:** Create the SQLAlchemy model for tracking video processing jobs.
    *   **Action(s):** Define the `VideoJobModel` class inheriting from `Base`.
    *   **Attributes:**
        *   `id`: `Column(Integer, primary_key=True, autoincrement=True)`
        *   `video_id`: `Column(Integer, ForeignKey("videos.id"), nullable=False)`
        *   `status`: `Column(SQLAlchemyEnumType(ProcessingStatus), nullable=False, default=ProcessingStatus.PENDING)`
        *   `processing_stages`: `Column(JSON, nullable=True)` (To store details/logs of different stages)
        *   `error_message`: `Column(Text, nullable=True)`
        *   `created_at`: `Column(DateTime, default=func.now(), nullable=False)`
        *   `updated_at`: `Column(DateTime, default=func.now(), onupdate=func.now(), nullable=False)`
    *   **Relationships:**
        *   `video = relationship("VideoModel", back_populates="jobs")` (Add `jobs = relationship("VideoJobModel", back_populates="video")` to `VideoModel`)
    *   **Key File(s):** `apps/core/models/video_job_model.py`
    *   **Verification/Deliverable(s):** `VideoJobModel` class correctly defined with attributes and relationship to `VideoModel`. `VideoModel` updated with the inverse relationship.

*   **Task 3.4: Define `VideoMetadataModel` in `apps/core/models/video_metadata_model.py`** [ ]
    *   **Objective:** Create the SQLAlchemy model for storing extracted metadata from videos.
    *   **Action(s):** Define the `VideoMetadataModel` class inheriting from `Base`.
    *   **Attributes:**
        *   `id`: `Column(Integer, primary_key=True, autoincrement=True)`
        *   `job_id`: `Column(Integer, ForeignKey("video_jobs.id"), unique=True, nullable=False)`
        *   `title`: `Column(String, nullable=True)`
        *   `description`: `Column(Text, nullable=True)`
        *   `tags`: `Column(JSON, nullable=True)` (Alternatively, `sqlalchemy.dialects.postgresql.ARRAY(String)` for PostgreSQL)
        *   `transcript_text`: `Column(Text, nullable=True)`
        *   `transcript_file_url`: `Column(String, nullable=True)` (URL to stored transcript file)
        *   `subtitle_files_urls`: `Column(JSON, nullable=True)` (e.g., `{"vtt": "url_to_vtt", "srt": "url_to_srt"}`)
        *   `thumbnail_file_url`: `Column(String, nullable=True)` (URL to stored thumbnail image)
        *   `extracted_video_duration_seconds`: `Column(Float, nullable=True)`
        *   `extracted_video_resolution`: `Column(String, nullable=True)` (e.g., "1920x1080")
        *   `extracted_video_format`: `Column(String, nullable=True)` (e.g., "mp4")
        *   `show_notes_text`: `Column(Text, nullable=True)`
        *   `created_at`: `Column(DateTime, default=func.now(), nullable=False)`
        *   `updated_at`: `Column(DateTime, default=func.now(), onupdate=func.now(), nullable=False)`
    *   **Relationships:**
        *   `job = relationship("VideoJobModel", back_populates="metadata")`
        *   (Add `metadata = relationship("VideoMetadataModel", back_populates="job", uselist=False)` to `VideoJobModel`)
    *   **Key File(s):** `apps/core/models/video_metadata_model.py`
    *   **Verification/Deliverable(s):** `VideoMetadataModel` class correctly defined. `VideoJobModel` updated with the inverse one-to-one relationship.

*   **Task 3.5: Configure Model Imports and Alembic Metadata** [ ]
    *   **Objective:** Ensure all defined models are accessible and recognized by Alembic.
    *   **Action(s):**
        1.  Update `apps/core/models/__init__.py` to import all model classes (e.g., `from .video_model import VideoModel`, `from .video_job_model import VideoJobModel`, etc.) and also `Base` from `apps.core.lib.database.connection`. This makes them available for Alembic's autogenerate feature.
        2.  Verify that `apps/core/alembic/env.py` has `target_metadata = Base.metadata` (where `Base` is imported from where your models are registered, typically via `models.__init__`).
    *   **Key File(s):** `apps/core/models/__init__.py`, `apps/core/alembic/env.py`
    *   **Verification/Deliverable(s):** `__init__.py` correctly exports models. Alembic `env.py` correctly points to the `Base.metadata` containing all model definitions.

*   **Task 3.6: Generate and Apply Database Migrations for Video Processing Tables** [ ]
    *   **Objective:** Create and apply database schema changes based on the newly defined models.
    *   **Action(s):**
        1.  Navigate to the `apps/core/` directory in your terminal.
        2.  Run Alembic to automatically generate a new revision script: `alembic revision --autogenerate -m "create_video_processing_tables"`
        3.  Carefully inspect the generated migration script located in `apps/core/alembic/versions/`. Verify it accurately reflects the intended schema changes for `VideoModel`, `VideoJobModel`, and `VideoMetadataModel`.
        4.  Apply the migration to your local Supabase database: `alembic upgrade head`
    *   **Verification/Deliverable(s):**
        *   A new Alembic migration script is generated.
        *   The migration is successfully applied to the local development database.
        *   The corresponding tables (`videos`, `video_jobs`, `video_metadata`) exist in the database with the correct columns and relationships.

#### **4. Develop/Enhance Core Libraries (`apps/core/lib/`)**

##### **Storage (`apps/core/lib/storage/file_storage.py`)**

*   **Task 4.1: Define `FileStorageService` Class Structure** [ ]
    *   **Objective:** Create the basic structure for the file storage service.
    *   **Action(s):** Define the `FileStorageService` class in `apps/core/lib/storage/file_storage.py`. The constructor should accept `settings: Settings` (from `apps.core.core.config`).
    *   **Code Structure:**
        ```python
        # apps/core/lib/storage/file_storage.py
        from abc import ABC, abstractmethod
        from apps.core.core.config import Settings
        from pathlib import Path
        import aiofiles
        import os
        # Add GCS client imports if/when GCS is implemented
        # from google.cloud import storage

        class BaseStorageService(ABC):
            @abstractmethod
            async def save_file(self, file_content: bytes, filename: str, subdir: str = "uploads") -> str:
                pass

            @abstractmethod
            async def download_file(self, storage_path: str, destination_local_path: str) -> None:
                pass
            
            @abstractmethod
            async def get_public_url(self, storage_path: str) -> Optional[str]:
                pass

        class LocalFileStorageService(BaseStorageService):
            def __init__(self, settings: Settings):
                self.settings = settings
                self.base_path = Path(settings.LOCAL_STORAGE_PATH)
                # Ensure base_path exists
                self.base_path.mkdir(parents=True, exist_ok=True)

            async def save_file(self, file_content: bytes, filename: str, subdir: str = "uploads") -> str:
                # ... implementation ...
                return str(file_path_abs) # Return absolute local path or path relative to a known root

            async def download_file(self, storage_path: str, destination_local_path: str) -> None:
                # ... implementation for local copy ...
                pass # storage_path is likely an absolute local path here

            async def get_public_url(self, storage_path: str) -> Optional[str]:
                # Local files typically don't have a direct public HTTP URL without a serving mechanism
                return None # Or a placeholder like f"file://{storage_path}" for dev reference

        # class GCSFileStorageService(BaseStorageService):
        #     def __init__(self, settings: Settings):
        #         self.settings = settings
        #         # Initialize GCS client, potentially using GOOGLE_APPLICATION_CREDENTIALS_PATH
        #         # self.client = storage.Client()
        #         # self.bucket = self.client.bucket(settings.GCS_BUCKET_NAME)
        #         pass # Placeholder
        #
        #     async def save_file(self, file_content: bytes, filename: str, subdir: str = "uploads") -> str:
        #         # ... GCS upload implementation ...
        #         # blob_path = f"{subdir}/{filename}"
        #         # blob = self.bucket.blob(blob_path)
        #         # await asyncio.to_thread(blob.upload_from_string, file_content)
        #         # return f"gs://{self.settings.GCS_BUCKET_NAME}/{blob_path}"
        #         pass # Placeholder
        #
        #     async def download_file(self, storage_path: str, destination_local_path: str) -> None:
        #         # ... GCS download implementation ...
        #         # Assuming storage_path is gs://bucket_name/path/to/blob
        #         pass # Placeholder
        #
        #     async def get_public_url(self, storage_path: str) -> Optional[str]:
        #         # ... GCS public URL generation ...
        #         # Assuming storage_path is gs://bucket_name/path/to/blob
        #         # blob_name = storage_path.replace(f"gs://{self.settings.GCS_BUCKET_NAME}/", "")
        #         # blob = self.bucket.blob(blob_name)
        #         # return blob.public_url # Or signed URL
        #         pass # Placeholder

        def get_file_storage_service(settings: Settings) -> BaseStorageService:
            if settings.STORAGE_BACKEND == "local":
                return LocalFileStorageService(settings)
            # elif settings.STORAGE_BACKEND == "gcs":
            #     if not settings.GCS_BUCKET_NAME:
            #         raise ValueError("GCS_BUCKET_NAME must be set for GCS storage backend.")
            #     return GCSFileStorageService(settings)
            else:
                raise ValueError(f"Unsupported storage backend: {settings.STORAGE_BACKEND}")

        ```
    *   **Key File(s):** `apps/core/lib/storage/file_storage.py`
    *   **Verification/Deliverable(s):** `FileStorageService` class structure with constructor and method signatures defined. `get_file_storage_service` factory function created.

*   **Task 4.2: Implement `save_file` Method for `FileStorageService`** [ ]
    *   **Objective:** Implement file saving logic for both local and GCS backends.
    *   **Action(s):**
        1.  **Local Storage:**
            *   In `LocalFileStorageService.save_file`:
                *   Construct the full file path: `{self.base_path}/{subdir}/{filename}`.
                *   Ensure the subdirectory `{self.base_path}/{subdir}` exists (use `os.makedirs(..., exist_ok=True)` or `Path(...).mkdir(parents=True, exist_ok=True)`).
                *   Asynchronously write `file_content` (bytes) to this path using `aiofiles`.
                *   Return the absolute local path of the saved file (e.g., `/abs/path/to/apps/core/output_files/subdir/filename`) or a path relative to a well-defined service root.
        2.  **GCS Storage (Placeholder/Future):**
            *   In `GCSFileStorageService.save_file` (if implementing now, otherwise placeholder):
                *   Use the `google-cloud-storage` client.
                *   Upload `file_content` to `self.settings.GCS_BUCKET_NAME` under the GCS path `{subdir}/{filename}`.
                *   Return the GCS URI (e.g., `gs://{bucket_name}/{subdir}/{filename}`).
    *   **Verification/Deliverable(s):** `save_file` method correctly saves files to the configured backend (initially local) and returns the storage path.

*   **Task 4.3: Implement `download_file` Method for `FileStorageService`** [ ]
    *   **Objective:** Implement file downloading logic.
    *   **Action(s):**
        1.  **Local Storage:**
            *   In `LocalFileStorageService.download_file`: `storage_path` will be an absolute local path. Copy the file from `storage_path` to `destination_local_path`. Can use `shutil.copy` or `aiofiles` for async copy.
        2.  **GCS Storage (Placeholder/Future):**
            *   In `GCSFileStorageService.download_file`: `storage_path` is a GCS URI. Download the blob to `destination_local_path`.
    *   **Verification/Deliverable(s):** `download_file` method correctly retrieves files from storage.

*   **Task 4.4: Implement `get_public_url` Method for `FileStorageService`** [ ]
    *   **Objective:** Implement logic to get a publicly accessible URL for a stored file.
    *   **Action(s):**
        1.  **Local Storage:**
            *   In `LocalFileStorageService.get_public_url`: Local files are not typically public via HTTP without a dedicated serving mechanism. Return `None` or a `file://` URI for dev reference if appropriate.
        2.  **GCS Storage (Placeholder/Future):**
            *   In `GCSFileStorageService.get_public_url`: Generate and return a public URL or a signed URL for the GCS object.
    *   **Verification/Deliverable(s):** `get_public_url` method returns an appropriate URL or `None`.


##### **AI (`apps/core/lib/ai/`)**

*   **Task 4.5: Define Base AI Adapter Interface (`apps/core/lib/ai/base_adapter.py`)** [ ]
    *   **Objective:** Create an abstract base class for AI service adapters.
    *   **Action(s):** Define `AIAdapterInterface(ABC)` with abstract methods:
        *   `async generate_text(self, prompt: str, context: Optional[str] = None) -> str`
        *   `async transcribe_audio(self, audio_file_path: str) -> str` (should aim to return structured transcript if possible, e.g., JSON with segments and timestamps, otherwise plain text)
    *   **Key File(s):** `apps/core/lib/ai/base_adapter.py`
    *   **Verification/Deliverable(s):** `AIAdapterInterface` defined with specified abstract methods.

*   **Task 4.6: Implement Gemini Adapter (`apps/core/lib/ai/gemini_adapter.py`)** [ ]
    *   **Objective:** Create a concrete implementation of `AIAdapterInterface` for Google Gemini.
    *   **Action(s):**
        1.  Define `GeminiAdapter(AIAdapterInterface)`.
        2.  Constructor `__init__(self, settings: Settings)`: Initialize the Gemini client using `settings.GEMINI_API_KEY`.
        3.  Implement `async generate_text` using the Gemini SDK.
        4.  Implement `async transcribe_audio` (if Gemini supports it directly for local files or via an intermediate GCS upload; otherwise, this might involve a different service or be a placeholder).
    *   **Key File(s):** `apps/core/lib/ai/gemini_adapter.py`
    *   **Verification/Deliverable(s):** `GeminiAdapter` implemented, capable of text generation and (potentially) audio transcription.

*   **Task 4.7: Create AI Client Factory (`apps/core/lib/ai/ai_client_factory.py`)** [ ]
    *   **Objective:** Create a factory function to provide an instance of an AI adapter.
    *   **Action(s):** Define `def get_ai_adapter(settings: Settings) -> AIAdapterInterface:`. Initially, it can return `GeminiAdapter(settings)`. This allows for easier swapping of AI providers later.
    *   **Key File(s):** `apps/core/lib/ai/ai_client_factory.py`
    *   **Verification/Deliverable(s):** `get_ai_adapter` factory function created.

*   **Task 4.8: Implement AI Caching with Redis (`apps/core/lib/cache/redis_cache.py`)** [ ]
    *   **Objective:** Set up a Redis-based caching mechanism for AI API calls to reduce costs and latency.
    *   **Action(s):**
        1.  Create `apps/core/lib/cache/redis_cache.py`.
        2.  Define a `RedisCache` class.
            *   Constructor `__init__(self, settings: Settings)`: Connect to Redis using `redis-py` (async version `redis.asyncio` recommended) with `settings.REDIS_HOST`, `settings.REDIS_PORT`, `settings.REDIS_DB`, `settings.REDIS_PASSWORD`.
            *   Implement `async get(self, key: str) -> Optional[str]:` (or `Optional[Any]` if storing complex objects, using `json.dumps/loads` or `pickle`).
            *   Implement `async set(self, key: str, value: str, ttl_seconds: Optional[int] = None):`
        3.  Modify AI adapter implementations (e.g., `GeminiAdapter`) to use this `RedisCache`.
            *   Before making an API call, attempt to retrieve the result from the cache using a key derived from the input (e.g., hash of the prompt and context).
            *   If a cache miss, make the API call, then store the result in the cache.
    *   **Key File(s):** `apps/core/lib/cache/redis_cache.py`, AI adapter files.
    *   **Verification/Deliverable(s):** `RedisCache` class implemented. AI adapters use caching for relevant API calls.

##### **Utilities (`apps/core/lib/utils/`)**

*   **Task 4.9: Implement FFmpeg Utilities (`apps/core/lib/utils/ffmpeg_utils.py`)** [ ]
    *   **Objective:** Create utility functions for video/audio manipulation using FFmpeg.
    *   **Action(s):**
        1.  Create `apps/core/lib/utils/ffmpeg_utils.py`.
        2.  Define `FfmpegUtils` class or standalone functions.
        3.  Implement methods using `subprocess.run` to call the `ffmpeg` command-line tool. Ensure `ffmpeg` is accessible in the system PATH.
            *   `extract_audio_sync(video_path: str, output_audio_path: str) -> None`: Extracts audio from video.
            *   `extract_frame_sync(video_path: str, timestamp_seconds: float, output_image_path: str) -> None`: Extracts a single frame.
            *   `get_video_metadata_sync(video_path: str) -> dict`: Uses `ffprobe` (part of FFmpeg) to get video metadata (duration, resolution, format, etc.) and returns it as a dictionary.
        4.  Consider error handling for `subprocess.run` calls (e.g., check return codes, capture stderr).
    *   **Key File(s):** `apps/core/lib/utils/ffmpeg_utils.py`
    *   **Verification/Deliverable(s):** FFmpeg utility functions capable of audio extraction, frame extraction, and metadata retrieval.

*   **Task 4.10: Implement File Handling Utilities (`apps/core/lib/utils/file_utils.py`)** [ ]
    *   **Objective:** Create utilities for common file system operations, especially temporary file/directory management.
    *   **Action(s):**
        1.  Create `apps/core/lib/utils/file_utils.py`.
        2.  Define `FileUtils` class or standalone functions.
            *   `create_temp_dir() -> str`: Creates a temporary directory using `tempfile.mkdtemp()` and returns its path.
            *   `cleanup_temp_dir(dir_path: str) -> None`: Removes the specified directory and its contents using `shutil.rmtree()`. Handle potential errors.
    *   **Key File(s):** `apps/core/lib/utils/file_utils.py`
    *   **Verification/Deliverable(s):** File utilities for temporary directory creation and cleanup.

*   **Task 4.11: Implement Subtitle Utilities (`apps/core/lib/utils/subtitle_utils.py`)** [ ]
    *   **Objective:** Create utilities for generating subtitle files (VTT, SRT) from transcript data.
    *   **Action(s):**
        1.  Create `apps/core/lib/utils/subtitle_utils.py`.
        2.  Define `SubtitleUtils` class or standalone functions.
        3.  Define the expected structure for `transcript_segments` (e.g., a list of dictionaries: `[{'text': '...', 'start_time': 0.5, 'end_time': 1.2}, ...]`).
        4.  Implement `generate_vtt(transcript_segments: list) -> str`: Generates VTT content as a string.
        5.  Implement `generate_srt(transcript_segments: list) -> str`: Generates SRT content as a string.
    *   **Key File(s):** `apps/core/lib/utils/subtitle_utils.py`
    *   **Verification/Deliverable(s):** Subtitle utilities capable of generating VTT and SRT formatted strings from structured transcript data.

##### **Authentication Utilities (`apps/core/lib/auth/supabase_auth.py`)**

*   **Task 4.12: Define `AuthenticatedUser` Model (`apps/core/lib/auth/supabase_auth.py`)** [ ]
    *   **Objective:** Create a Pydantic model to represent an authenticated user based on Supabase JWT claims.
    *   **Action(s):** Define `AuthenticatedUser(BaseModel)`:
        *   `id: str` (maps to `sub` claim)
        *   `email: Optional[str] = None`
        *   `aud: Optional[str] = None` (audience)
        *   Other relevant claims as needed (e.g., `role`).
    *   **Key File(s):** `apps/core/lib/auth/supabase_auth.py`
    *   **Verification/Deliverable(s):** `AuthenticatedUser` Pydantic model defined.

*   **Task 4.13: Implement `get_current_user` Dependency (`apps/core/lib/auth/supabase_auth.py`)** [ ]
    *   **Objective:** Create a FastAPI dependency to authenticate users using Supabase JWTs.
    *   **Action(s):**
        1.  Define `async def get_current_user(settings: Settings = Depends(get_settings_dependency), token: str = Depends(OAuth2PasswordBearer(tokenUrl="api/v1/auth/token"))) -> AuthenticatedUser:`
            *   Note: `tokenUrl` is a placeholder here as Supabase client typically handles token acquisition. It's needed by `OAuth2PasswordBearer` but won't be called by FastAPI itself if tokens are passed in `Authorization` header. A dummy endpoint can be created if strict OpenAPI validation requires it.
            *   Use `python-jose.jwt.decode` to validate and decode the token.
                *   Key: `settings.SUPABASE_JWT_SECRET`
                *   Algorithms: `["HS256"]` (Confirm Supabase default)
                *   Audience: `"authenticated"` (Confirm Supabase default audience for user tokens)
            *   Extract `sub` (as user ID), `email`, `aud`, and any other relevant claims from the decoded token.
            *   Populate and return an `AuthenticatedUser` model.
            *   Handle `jose.JWTError` or `jose.ExpiredSignatureError` by raising an `HTTPException` (e.g., status code 401 or 403).
        2.  Create `get_settings_dependency` if not already available, e.g. `def get_settings_dependency() -> Settings: return settings_instance_from_config_py`.
    *   **Key File(s):** `apps/core/lib/auth/supabase_auth.py`
    *   **Verification/Deliverable(s):** `get_current_user` FastAPI dependency that can validate a Supabase JWT and return user information.

#### **5. Error Handling (`apps/core/core/exceptions.py`)**

*   **Task 5.1: Define Custom Application Exceptions** [ ]
    *   **Objective:** Create custom exception classes for domain-specific errors.
    *   **Action(s):** Define the following custom exceptions in `apps/core/core/exceptions.py`:
        *   `VideoProcessingError(Exception)`: Base exception for video processing issues.
        *   `AINoResponseError(VideoProcessingError)`: For errors when AI services fail to respond.
        *   `FFmpegError(VideoProcessingError)`: For errors originating from FFmpeg command execution.
        *   `StorageServiceError(Exception)`: Base for storage related issues.
        *   `ConfigurationError(Exception)`: For issues with application configuration.
    *   **Key File(s):** `apps/core/core/exceptions.py`
    *   **Verification/Deliverable(s):** Custom exception classes defined.

---

### **Phase 2: Data Access - Operations Layer (Repositories)**

    *   **General Requirement:** All repository methods should accept `db: Session` (SQLAlchemy session) as their first argument and are typically called from service layer methods.

#### **6. Implement Repositories (`apps/core/operations/`)**

*   **Task 6.1: Implement `VideoRepository` (`apps/core/operations/video_repository.py`)** [ ]
    *   **Objective:** Create a repository for CRUD operations on `VideoModel`.
    *   **Action(s):** Define `VideoRepository` class.
    *   **Methods:**
        *   `create_video(self, db: Session, *, uploader_user_id: str, original_filename: str, storage_path: str, content_type: str, size_bytes: int) -> VideoModel`: Creates and returns a new `VideoModel` instance.
        *   `get_video_by_id(self, db: Session, *, video_id: int) -> Optional[VideoModel]`: Retrieves a video by its ID.
        *   `get_videos_by_uploader(self, db: Session, *, uploader_user_id: str, skip: int = 0, limit: int = 100) -> List[VideoModel]`: Retrieves videos for a specific uploader.
    *   **Key File(s):** `apps/core/operations/video_repository.py`
    *   **Verification/Deliverable(s):** `VideoRepository` with implemented CRUD methods.

*   **Task 6.2: Implement `VideoJobRepository` (`apps/core/operations/video_job_repository.py`)** [ ]
    *   **Objective:** Create a repository for operations on `VideoJobModel`.
    *   **Action(s):** Define `VideoJobRepository` class.
    *   **Methods:**
        *   `create_video_job(self, db: Session, *, video_id: int, initial_status: ProcessingStatus = ProcessingStatus.PENDING) -> VideoJobModel`: Creates a new video job.
        *   `get_video_job_by_id(self, db: Session, *, job_id: int) -> Optional[VideoJobModel]`: Retrieves a job by ID.
        *   `update_job_status(self, db: Session, *, job_id: int, status: ProcessingStatus, error_message: Optional[str] = None) -> Optional[VideoJobModel]`: Updates job status and optionally an error message.
        *   `add_processing_stage_log(self, db: Session, *, job_id: int, stage_name: str, details: dict) -> Optional[VideoJobModel]`: Appends a log entry to the `processing_stages` JSON field. (Requires careful handling of JSON updates in SQLAlchemy).
        *   `get_jobs_for_video(self, db: Session, *, video_id: int) -> List[VideoJobModel]`: Retrieves all jobs for a video.
    *   **Key File(s):** `apps/core/operations/video_job_repository.py`
    *   **Verification/Deliverable(s):** `VideoJobRepository` with implemented methods.

*   **Task 6.3: Implement `VideoMetadataRepository` (`apps/core/operations/video_metadata_repository.py`)** [ ]
    *   **Objective:** Create a repository for operations on `VideoMetadataModel`.
    *   **Action(s):** Define `VideoMetadataRepository` class.
    *   **Methods:**
        *   `create_or_update_metadata(self, db: Session, *, job_id: int, **kwargs) -> VideoMetadataModel`: Creates new metadata or updates existing metadata for a given `job_id`.
            *   This method should first try to fetch metadata by `job_id`. If it exists, update its fields with `**kwargs`. If not, create a new `VideoMetadataModel` instance.
        *   `get_metadata_by_job_id(self, db: Session, *, job_id: int) -> Optional[VideoMetadataModel]`: Retrieves metadata by job ID.
    *   **Key File(s):** `apps/core/operations/video_metadata_repository.py`
    *   **Verification/Deliverable(s):** `VideoMetadataRepository` with implemented methods.

---

### **Phase 3: Business Logic - Service Layer**

#### **7. Develop Core Video Processing Service (`apps/core/services/video_processing_service.py`)**

*   **Task 7.1: Define `VideoProcessingService` Class Structure and Dependencies** [ ]
    *   **Objective:** Set up the main service class for handling video processing logic.
    *   **Action(s):**
        1.  Define `VideoProcessingService` class in `apps/core/services/video_processing_service.py`.
        2.  The constructor `__init__` should inject instances of:
            *   `VideoRepository`
            *   `VideoJobRepository`
            *   `VideoMetadataRepository`
            *   `BaseStorageService` (from `apps.core.lib.storage.file_storage`)
            *   `AIAdapterInterface` (from `apps.core.lib.ai.ai_client_factory`)
            *   `FfmpegUtils` (or its functions)
            *   `SubtitleUtils` (or its functions)
            *   `FileUtils` (or its functions)
            *   `Settings` (from `apps.core.core.config`)
    *   **Key File(s):** `apps/core/services/video_processing_service.py`
    *   **Verification/Deliverable(s):** `VideoProcessingService` class defined with a constructor injecting all necessary dependencies.

*   **Task 7.2: Implement `initiate_video_processing` Method** [ ]
    *   **Objective:** Create the service method to start the video processing workflow.
    *   **Action(s):** Implement `async def initiate_video_processing(self, db: Session, original_filename: str, video_content: bytes, content_type: str, uploader_user_id: str, background_tasks: BackgroundTasks) -> VideoJobModel`:
        1.  Generate a unique filename (e.g., using `uuid.uuid4()`) to avoid collisions.
        2.  Save the `video_content` using `self.file_storage_service.save_file()`. The `subdir` could be structured, e.g., `f"videos/{uploader_user_id}/{unique_filename_stem}"`.
        3.  Create a `VideoModel` record in the database using `self.video_repository.create_video()`.
        4.  Create a `VideoJobModel` record (initial status `PENDING`) using `self.video_job_repository.create_video_job()`, linking it to the created `VideoModel`.
        5.  Add the background processing task: `background_tasks.add_task(self._execute_processing_pipeline, job_id=new_job.id, video_storage_path=stored_video_path, video_original_filename=original_filename)`.
        6.  Perform `db.commit()` to save the initial `VideoModel` and `VideoJobModel` records.
        7.  Return the newly created `VideoJobModel`.
    *   **Verification/Deliverable(s):** `initiate_video_processing` method implemented, correctly saving the video file, creating database records, and scheduling the background processing task.

*   **Task 7.3: Implement `_execute_processing_pipeline` Background Method** [ ]
    *   **Objective:** Implement the core logic for processing a video in the background.
    *   **Action(s):** Implement `async def _execute_processing_pipeline(self, job_id: int, video_storage_path: str, video_original_filename: str)`:
        *   **Important:** This method runs in a background task, so it needs its own database session. Use a `with get_db_session() as db_bg:` context (assuming `get_db_session` is a context manager yielding a session).
        1.  Fetch the `VideoJobModel` using `job_id` via `self.video_job_repository`. If not found, log an error and exit.
        2.  Update job status to `PROCESSING` using `self.video_job_repository.update_job_status()`. Commit `db_bg`.
        3.  Create a temporary working directory using `self.file_utils.create_temp_dir()`.
        4.  **Implement a `try...except...finally` block for robust processing and cleanup:**
            *   **`try` block:**
                1.  Download the video file from `video_storage_path` to a path within the temporary directory using `self.file_storage_service.download_file()`. Let this be `local_video_path`.
                2.  **Step 1: Basic Video Metadata Extraction:**
                    *   Use `self.ffmpeg_utils.get_video_metadata_sync(local_video_path)`.
                    *   Update `VideoMetadataModel` via `self.video_metadata_repository.create_or_update_metadata()` with extracted duration, resolution, format. Commit `db_bg`.
                3.  **Step 2: Audio Extraction:**
                    *   Define an `output_audio_path` in the temp directory.
                    *   Use `self.ffmpeg_utils.extract_audio_sync(local_video_path, output_audio_path)`.
                4.  **Step 3: Audio Transcription:**
                    *   Use `self.ai_adapter.transcribe_audio(output_audio_path)`. This should return structured transcript data (segments with timestamps) if possible.
                    *   Save the raw transcript text to `VideoMetadataModel.transcript_text`.
                    *   Save the full transcript (text or structured JSON) as a `.txt` or `.json` file. Upload this file using `self.file_storage_service.save_file()` (e.g., in `transcripts/job_{job_id}/transcript.txt`). Store its path/URL in `VideoMetadataModel.transcript_file_url`. Commit `db_bg`.
                5.  **Step 4: Content Metadata Generation (Title, Description, Tags, Show Notes):**
                    *   Prepare a prompt for `self.ai_adapter.generate_text()`, possibly using parts of the transcript as context.
                    *   Request generation of: Title, Description, Tags (as a list), Show Notes.
                    *   Parse the AI response and update the corresponding fields in `VideoMetadataModel`. Commit `db_bg`.
                6.  **Step 5: Subtitle Generation:**
                    *   If structured transcript (with timestamps) is available from Step 3:
                        *   Generate VTT content: `vtt_content = self.subtitle_utils.generate_vtt(transcript_segments)`.
                        *   Generate SRT content: `srt_content = self.subtitle_utils.generate_srt(transcript_segments)`.
                        *   Save VTT and SRT files. Upload them via `self.file_storage_service.save_file()` (e.g., `subtitles/job_{job_id}/`).
                        *   Store their paths/URLs in `VideoMetadataModel.subtitle_files_urls` (e.g., `{"vtt": "path/url", "srt": "path/url"}`). Commit `db_bg`.
                7.  **Step 6: Thumbnail Extraction:**
                    *   Determine a suitable timestamp for thumbnail (e.g., 10% into the video, or a specific time).
                    *   Define an `output_thumbnail_path` in the temp directory (e.g., `thumbnail.jpg`).
                    *   Use `self.ffmpeg_utils.extract_frame_sync(local_video_path, timestamp_seconds, output_thumbnail_path)`.
                    *   Upload the thumbnail using `self.file_storage_service.save_file()` (e.g., `thumbnails/job_{job_id}/thumbnail.jpg`).
                    *   Store its path/URL in `VideoMetadataModel.thumbnail_file_url`. Commit `db_bg`.
                8.  Update job status to `COMPLETED` via `self.video_job_repository.update_job_status()`. Commit `db_bg`.
            *   **`except Exception as e:` block:**
                *   Log the error thoroughly (e.g., `logger.error(f"Error processing job {job_id}: {e}", exc_info=True)`).
                *   Update job status to `FAILED` with the error message `str(e)` via `self.video_job_repository.update_job_status()`. Commit `db_bg`.
            *   **`finally` block:**
                *   Clean up the temporary working directory using `self.file_utils.cleanup_temp_dir()`.
    *   **Verification/Deliverable(s):** `_execute_processing_pipeline` method implemented, performing all video processing steps, updating database records correctly, and handling errors gracefully.

*   **Task 7.4: Implement `get_job_details` Method** [ ]
    *   **Objective:** Create a service method to retrieve details of a specific processing job, ensuring ownership.
    *   **Action(s):** Implement `async def get_job_details(self, db: Session, job_id: int, uploader_user_id: str) -> Optional[VideoJobModel]`:
        1.  Fetch the `VideoJobModel` by `job_id` using `self.video_job_repository.get_video_job_by_id()`. Ensure related `video` and `metadata` are loaded (e.g., using SQLAlchemy joined loading or selectinload options in the repository query).
        2.  If the job is found, verify ownership: check if `job.video.uploader_user_id == uploader_user_id`.
        3.  If job exists and user is the owner, return the `VideoJobModel`. Otherwise, return `None` (the endpoint layer will handle 404 or 403).
    *   **Verification/Deliverable(s):** `get_job_details` method implemented, correctly fetching job details and verifying ownership.

#### **8. Supporting Services (Optional/As Needed)**

*   **Task 8.1: Implement `UserService` (Example, if needed for local profiles)** [ ]
    *   **Objective:** Manage local user profiles if they are distinct from Supabase Auth users (e.g., for additional app-specific user data).
    *   **Action(s):** If a separate `UserModel` exists locally (beyond just using Supabase user IDs):
        1.  Create `apps/core/services/user_service.py`.
        2.  Define `UserService` with methods like `async def get_or_create_user_profile(db: Session, auth_user: AuthenticatedUser) -> UserModel`. This could be called after successful authentication to ensure a local user profile linked to the Supabase `auth_user.id` exists or is created.
    *   **Note:** This is often not strictly necessary if all user-related data can be linked directly via `uploader_user_id` from the `AuthenticatedUser` object.
    *   **Verification/Deliverable(s):** `UserService` implemented if required by the application design.

---

### **Phase 4: API Layer - Endpoints and Schemas**

#### **9. Define API Schemas (`apps/core/api/schemas/`)**

*   **Task 9.1: Create Schema Files (`video_processing_schemas.py`, etc.)** [ ]
    *   **Objective:** Organize Pydantic schemas for API request/response validation and serialization.
    *   **Action(s):** Create `apps/core/api/schemas/video_processing_schemas.py`. If a `UserService` and local `UserModel` are used, also create `user_schemas.py`.
    *   **Key File(s):** `apps/core/api/schemas/video_processing_schemas.py`
    *   **Verification/Deliverable(s):** Schema files created.

*   **Task 9.2: Define `VideoUploadResponseSchema`** [ ]
    *   **Objective:** Define the response schema for the video upload endpoint.
    *   **Action(s):** In `video_processing_schemas.py`, define `VideoUploadResponseSchema(BaseModel)`:
        *   `job_id: int`
        *   `status: ProcessingStatus` (from `apps.core.models.enums`)
        *   `message: str` (e.g., "Video upload accepted, processing initiated.")
    *   **Verification/Deliverable(s):** `VideoUploadResponseSchema` defined.

*   **Task 9.3: Define `VideoSchema`** [ ]
    *   **Objective:** Define a Pydantic schema for `VideoModel` data.
    *   **Action(s):** In `video_processing_schemas.py`, define `VideoSchema(BaseModel)` mirroring fields from `VideoModel`.
        *   Include `id: int`, `uploader_user_id: str`, `original_filename: str`, `storage_path: str`, `content_type: str`, `size_bytes: int`, `created_at: datetime`, `updated_at: datetime`.
        *   Add `model_config = ConfigDict(from_attributes=True)` to enable ORM mode.
    *   **Verification/Deliverable(s):** `VideoSchema` defined.

*   **Task 9.4: Define `VideoMetadataSchema`** [ ]
    *   **Objective:** Define a Pydantic schema for `VideoMetadataModel` data.
    *   **Action(s):** In `video_processing_schemas.py`, define `VideoMetadataSchema(BaseModel)` mirroring fields from `VideoMetadataModel`.
        *   Include all relevant fields like `title`, `description`, `tags`, `transcript_text_url`, `thumbnail_url`, etc.
        *   Add `model_config = ConfigDict(from_attributes=True)`.
    *   **Verification/Deliverable(s):** `VideoMetadataSchema` defined.

*   **Task 9.5: Define `VideoJobSchema` (for Job Details Response)** [ ]
    *   **Objective:** Define a comprehensive Pydantic schema for `VideoJobModel` data, including related objects.
    *   **Action(s):** In `video_processing_schemas.py`, define `VideoJobSchema(BaseModel)`:
        *   `id: int`
        *   `status: ProcessingStatus`
        *   `processing_stages: Optional[dict] = None`
        *   `error_message: Optional[str] = None`
        *   `created_at: datetime`
        *   `updated_at: datetime`
        *   `video: Optional[VideoSchema] = None` (nested schema for related video)
        *   `metadata: Optional[VideoMetadataSchema] = None` (nested schema for related metadata)
        *   Add `model_config = ConfigDict(from_attributes=True)`.
    *   **Verification/Deliverable(s):** `VideoJobSchema` defined.

#### **10. Create API Endpoints (`apps/core/api/endpoints/video_processing_endpoints.py`)**

*   **Task 10.1: Create API Router** [ ]
    *   **Objective:** Set up an APIRouter for video processing related endpoints.
    *   **Action(s):** In `apps/core/api/endpoints/video_processing_endpoints.py`, create `router = APIRouter()`.
    *   **Key File(s):** `apps/core/api/endpoints/video_processing_endpoints.py`
    *   **Verification/Deliverable(s):** APIRouter instance created.

*   **Task 10.2: Implement `POST /upload` Endpoint** [ ]
    *   **Objective:** Create the API endpoint for uploading videos and initiating processing.
    *   **Action(s):** Define `POST /upload` endpoint on the router.
        *   `response_model=VideoUploadResponseSchema`
        *   `status_code=status.HTTP_202_ACCEPTED`
        *   **Dependencies:**
            *   `current_user: AuthenticatedUser = Depends(get_current_user)`
            *   `db: Session = Depends(get_db_session)`
            *   `background_tasks: BackgroundTasks`
            *   `video_processing_service: VideoProcessingService = Depends(get_video_processing_service_dependency)` (You'll need to create `get_video_processing_service_dependency` that initializes and returns `VideoProcessingService` with its own dependencies).
            *   `file: UploadFile = File(...)`
        *   **Logic:**
            1.  Read video content: `video_content = await file.read()`.
            2.  Call `video_processing_service.initiate_video_processing()` with `file.filename`, `video_content`, `file.content_type`, `current_user.id`, and `background_tasks`.
            3.  Return the response using `VideoUploadResponseSchema`.
    *   **Verification/Deliverable(s):** `/upload` endpoint implemented, accepting file uploads, initiating processing, and returning the correct response.

*   **Task 10.3: Implement `GET /jobs/{job_id}` Endpoint** [ ]
    *   **Objective:** Create the API endpoint to retrieve the status and details of a specific video processing job.
    *   **Action(s):** Define `GET /jobs/{job_id}` endpoint on the router.
        *   `response_model=VideoJobSchema`
        *   **Dependencies:**
            *   `job_id: int` (path parameter)
            *   `current_user: AuthenticatedUser = Depends(get_current_user)`
            *   `db: Session = Depends(get_db_session)`
            *   `video_processing_service: VideoProcessingService = Depends(get_video_processing_service_dependency)`
        *   **Logic:**
            1.  Call `video_processing_service.get_job_details(db=db, job_id=job_id, uploader_user_id=current_user.id)`.
            2.  If the job is not found or the user is not the owner, raise `HTTPException(status_code=404, detail="Job not found or not authorized")`.
            3.  Return the job details.
    *   **Verification/Deliverable(s):** `/jobs/{job_id}` endpoint implemented, returning job details for authorized users.

*   **Task 10.4: Register Video Processing Router in `apps/core/main.py`** [ ]
    *   **Objective:** Make the video processing API endpoints accessible through the main FastAPI application.
    *   **Action(s):**
        1.  In `apps/core/main.py` (your main FastAPI app file):
            *   Import the video processing router: `from apps.core.api.endpoints import video_processing_endpoints`
            *   Include the router in the FastAPI app: `app.include_router(video_processing_endpoints.router, prefix="/api/v1/videos", tags=["Video Processing"])` (Adjust prefix as per `settings.API_PREFIX`).
    *   **Key File(s):** `apps/core/main.py`
    *   **Verification/Deliverable(s):** Video processing router correctly included in the main FastAPI application, and endpoints are accessible.

---

### **Phase 5: Testing, Documentation, and Finalization**

#### **11. Testing (`apps/core/tests/`)**

*   **Task 11.1: Implement Unit Tests (`apps/core/tests/unit/`)** [ ]
    *   **Objective:** Write unit tests for individual components like models, library functions, repository methods (mocking DB interactions), and service methods (mocking repository/library dependencies).
    *   **Action(s):**
        *   Test model validations and relationships.
        *   Test utility functions with various inputs.
        *   Test repository methods by mocking `db.Session` calls (`add`, `commit`, `query`, `filter`, etc.) and asserting correct behavior.
        *   Test service layer logic by mocking calls to repositories and other external services (like AI adapters, file storage).
    *   **Key Directory:** `apps/core/tests/unit/`
    *   **Verification/Deliverable(s):** A suite of unit tests covering core logic, with good code coverage.

*   **Task 11.2: Implement Integration Tests (`apps/core/tests/integration/api/`)** [ ]
    *   **Objective:** Write integration tests for the API endpoints, interacting with a test database and (optionally) mocked external services.
    *   **Action(s):**
        1.  Use FastAPI's `TestClient`.
        2.  Set up a test database (e.g., a separate local Supabase database or an in-memory SQLite for simpler tests if ORM allows). Override the `get_db_session` dependency for tests to point to this test database.
        3.  Override the `get_current_user` dependency to return a mock `AuthenticatedUser` for testing authenticated endpoints.
        4.  Test `POST /upload` endpoint:
            *   Upload actual test video files.
            *   Verify database records (`VideoModel`, `VideoJobModel`) are created.
            *   For background tasks: either mock the `_execute_processing_pipeline` method directly to prevent it from running, or if testing the pipeline itself, check DB for processing results after a delay (can be complex and slow for automated tests, consider focused tests for pipeline stages).
        5.  Test `GET /jobs/{job_id}` endpoint:
            *   Verify correct job details are returned for authorized users.
            *   Test unauthorized access.
        6.  Configure AI and Storage services to use local/mocked backends (e.g., `LocalFileStorageService`, a mock AI adapter) for integration tests to avoid real external calls and costs.
    *   **Key Directory:** `apps/core/tests/integration/api/`
    *   **Verification/Deliverable(s):** A suite of integration tests covering API endpoint functionality, including successful paths and error conditions.

#### **12. Documentation and Cleanup**

*   **Task 12.1: Update `apps/core/README.md`** [ ]
    *   **Objective:** Provide clear documentation for setting up and running the `apps/core` service.
    *   **Action(s):** Update or create `apps/core/README.md` with:
        *   Instructions for local setup (Python environment, dependencies, Supabase CLI, `.env` configuration).
        *   Overview of API endpoints (brief description, path, method).
        *   Environment variables required.
        *   How to run the application and tests.
    *   **Key File(s):** `apps/core/README.md`
    *   **Verification/Deliverable(s):** Comprehensive README for the `apps/core` application.

*   **Task 12.2: Add Python Docstrings** [ ]
    *   **Objective:** Improve code maintainability and understanding with comprehensive docstrings.
    *   **Action(s):** Add Python docstrings to all modules, classes, methods, and functions, explaining their purpose, arguments, and return values (where applicable). Follow a consistent docstring format (e.g., Google style, reStructuredText).
    *   **Verification/Deliverable(s):** Codebase is well-documented with docstrings.

*   **Task 12.3: Securely Remove Old `video_processor` Directory (LATER STAGE)** [ ]
    *   **Objective:** Clean up the old, now-refactored video processing code.
    *   **Action(s):** **After thorough verification and confirmation that the new `apps/core` system is fully functional and stable**, securely remove the old `apps/core/video_processor` directory (or its original location).
    *   **Caution:** This step should only be done once the new implementation is proven reliable. Consider version controlling the removal.
    *   **Verification/Deliverable(s):** Old `video_processor` code removed from the codebase.

*   **Task 12.4: Update This Project Plan Document (`.ai_docs/progress.md` or similar)** [ ]
    *   **Objective:** Maintain an accurate record of project progress.
    *   **Action(s):** As tasks are completed, update their status markers (e.g., `[ ]` to `[x]`) in this document (or its source file). Add notes or observations if necessary.
    *   **Verification/Deliverable(s):** This project plan document accurately reflects the current status of the refactoring project.

---

**Conclusion (Exemplar):** Upon completion of all phases and tasks outlined in this exemplar plan, the video processing functionality would be successfully refactored into the `apps/core` architecture, leveraging modern practices, Supabase integration, and a robust testing suite.

---

================
File: prompts/07-tests.md
================
---
title: "Test Plan & Code Generation from Completed Tasks"
description: "This prompt guides an AI to act as a Test Engineer. It analyzes a project plan (from 06-tasks-prompt.md) to identify completed tasks and then generates unit tests, integration tests, and behavioral test scenarios for those functionalities."
version: 1.0
author: AI Assistant
---

You are a meticulous Test Engineer / QA Automation Specialist. Your primary responsibility is to ensure the quality and correctness of the implemented features by developing comprehensive test plans and generating test code.

**Input:**

You will receive the detailed project execution plan and task list (the content of `06-tasks-prompt.md`) in the following block. This document contains tasks with their completion status indicated by checkboxes (e.g., `[x]` for complete, `[ ]` for incomplete).

```
<prev_step>
{Content of 06-tasks-prompt.md goes here}
</prev_step>
```

**Your Mission:**

1.  **Analyze Completed Tasks:** Carefully parse the provided project plan from `<prev_step>`. Identify all tasks marked as complete (`[x]`). Pay close attention to their "Objective," "Action(s)," and especially "Verification/Deliverable(s)" to understand what functionality was implemented and should now be tested.

2.  **Group Tasks for Testing:** Instead of creating tests for every single atomic task, group related completed tasks into logical units of functionality. For example:
    *   All tasks related to defining a specific data model and its repository methods form a testable unit for unit tests.
    *   Tasks implementing a service method and its underlying repository calls form another unit.
    *   Tasks for an API endpoint, its associated service logic, and schema definitions form a unit for integration tests.

3.  **Develop a Test Strategy & Generate Tests for Each Group:** For each logical group of completed functionalities, provide the following:

    *   **A. Relevant Completed Task IDs:** List the Task ID(s) from `06-tasks-prompt.md` that this set of tests will cover.
    *   **B. Brief Test Strategy:** Outline your approach:
        *   **Unit Tests:** Specify which modules, classes, or functions require unit tests. Name the target files (e.g., `apps/core/models/video_model.py`) and the specific methods/logic to test (e.g., model validation, utility function correctness).
        *   **Integration Tests:** Identify interactions between components that need testing (e.g., API endpoint -> Service -> Repository). Specify how to mock external dependencies ONLY IF ABSOLUTELY NECESSARY (prefer testing with real local instances like a test database if feasible and configured in the project setup tasks from `06-tasks-prompt.md`).
        *   **Behavioral/E2E Test Scenarios (High-Level):** Describe user stories or end-to-end flows that are now testable due to the completed tasks (e.g., "User uploads a video, processing completes, and metadata is viewable via API"). You don't need to generate full E2E automation code unless the task was specifically about UI testing tools like Selenium/Playwright.

    *   **C. Generated Test Code:**
        *   Generate `pytest`-compatible Python test code.
        *   Provide clear file paths for where these tests should reside (e.g., `apps/core/tests/unit/test_video_model.py`, `apps/core/tests/integration/api/test_video_processing_endpoints.py`).
        *   **Unit Tests:** Include necessary imports, mock setup (using `unittest.mock.patch` or `pytest-mock` for external dependencies not part of the unit), and detailed assertions.
        *   **Integration Tests (API):** Use FastAPI's `TestClient`. Show setup, how to make requests (including authentication if the relevant auth setup tasks in `06-tasks-prompt.md` are complete and a mock user can be injected), and how to assert responses and potential database state changes (if a test database is configured).
        *   Make test code clear, readable, and robust.

4.  **Prioritize Critical Paths:** Focus testing efforts on the most critical functionalities and business logic indicated by the completed tasks.

**Output Format:**

Structure your response in Markdown. For each logical group of completed tasks you identify, use the following template:

```markdown
## Test Suite for: [Descriptive Name of Functionality Group, e.g., Video Model & Repository]

**A. Relevant Completed Task IDs (from `06-tasks-prompt.md`):**
*   Task X.Y
*   Task X.Z

**B. Brief Test Strategy:**
*   **Unit Tests:**
    *   Target File(s): `path/to/your/module.py`
    *   Focus: Test `function_a` for valid inputs, `ClassB` methods for X, Y, Z scenarios.
    *   Mocks: Mock `external_service_dependency` if it's outside the unit.
*   **Integration Tests:**
    *   Target Endpoint(s)/Service(s): `POST /api/resource`, `ResourceService.process_data()`
    *   Focus: Verify end-to-end flow from API request to database change. Test with a local test database.
    *   Mocks: Mock external AI services for predictable responses if they are part of this flow but not the primary focus of testing this *integration*.
*   **Behavioral/E2E Test Scenarios:**
    1.  Scenario: User successfully creates a new resource via the UI/API, and it appears in the resource list.
    2.  Scenario: User attempts to create a resource with invalid data and receives appropriate error messages.

**C. Generated Test Code:**

### Unit Tests

**Python Example: File:** `apps/core/tests/unit/test_module_name.py`

```python
# Your generated Python unit test code here
import pytest
from unittest.mock import patch # or from pytest_mock import mocker

# Example:
# from apps.core.models.your_model import YourModel
# def test_your_model_creation():
#     instance = YourModel(name="Test")
#     assert instance.name == "Test"
```

**TypeScript Example: File:** `src/tests/unit/someModule.test.ts`

```typescript
// Your generated TypeScript unit test code here
// Example using Jest

// Assuming you have a function in src/utils/calculator.ts:
// export const add = (a: number, b: number): number => a + b;

import { add } from '../utils/calculator'; // Adjust path as needed

describe('Calculator', () => {
  describe('add function', () => {
    it('should return the sum of two numbers', () => {
      expect(add(2, 3)).toBe(5);
    });

    it('should handle negative numbers', () => {
      expect(add(-1, -1)).toBe(-2);
    });
  });
});

// Example for a class:
// // Assuming src/services/userService.ts
// export class UserService {
//   private users: { id: number; name: string }[] = [];
//   constructor() { this.users = []; }
//   addUser(id: number, name: string) { this.users.push({ id, name }); }
//   getUser(id: number) { return this.users.find(u => u.id === id); }
// }
//
// import { UserService } from '../services/userService'; // Adjust
// describe('UserService', () => {
//   let userService: UserService;
//   beforeEach(() => { userService = new UserService(); });
//
//   it('should add a user', () => {
//     userService.addUser(1, 'Alice');
//     expect(userService.getUser(1)).toEqual({ id: 1, name: 'Alice' });
//   });
// });
```

### Integration Tests

**Python Example (FastAPI): File:** `apps/core/tests/integration/api/test_api_endpoint.py`

```python
# Your generated Python integration test code here
import pytest
from fastapi.testclient import TestClient

# Example:
# from apps.core.main import app # Assuming your FastAPI app instance is here
# client = TestClient(app)
#
# def test_create_resource():
#     response = client.post("/api/v1/resource", json={"name": "Test Resource"})
#     assert response.status_code == 201 # Or 200, 202 etc. based on endpoint
#     data = response.json()
#     assert data["name"] == "Test Resource"
#     # Add assertions for database state if applicable
```

**TypeScript Example (Node.js/Express with Supertest & Jest): File:** `src/tests/integration/api.integration.test.ts`

```typescript
// Your generated TypeScript integration test code here
// Example for an Express.js API using Jest and Supertest

// import request from 'supertest';
// import { app } from '../app'; // Assuming your Express app instance is exported

// describe('API Endpoints', () => {
//   describe('POST /api/items', () => {
//     it('should create a new item', async () => {
//       const newItem = { name: 'Test Item', value: 100 };
//       const response = await request(app)
//         .post('/api/items')
//         .send(newItem);
//
//       expect(response.status).toBe(201); // Or your success status code
//       expect(response.body).toHaveProperty('id');
//       expect(response.body.name).toBe(newItem.name);
//       // Potentially check database state here if feasible
//     });
//   });

//   describe('GET /api/items/:id', () => {
//     it('should retrieve an existing item', async () => {
//       // Assume an item with ID 1 exists, or create one first
//       // const itemId = 1;
//       // const response = await request(app).get(`/api/items/${itemId}`);
//       //
//       // expect(response.status).toBe(200);
//       // expect(response.body).toHaveProperty('id', itemId);
//       // Placeholder for actual test
//       expect(true).toBe(true);
//     });
//   });
// });
```

---
```

**Important Considerations:**
*   **Test Data:** For tests requiring specific data, mention the type of test data needed or provide simple examples within the test code itself.
*   **Configuration:** Assume that test configurations (like a separate test database connection string or mocked service endpoints) are handled as per the project setup tasks in `06-tasks-prompt.md` or standard testing practices (e.g., using `pytest` fixtures or environment variables for tests).
*   **Clarity over Quantity:** Well-structured, meaningful tests are more valuable than a large number of superficial ones.
*   **Incomplete Tasks:** Do NOT generate tests for tasks marked `[ ]` (incomplete). Only focus on functionality verified by `[x]` tasks.

Your goal is to provide a practical and actionable set of tests that can be directly integrated into the project's CI/CD pipeline to maintain high quality.

================
File: tests/integration/test_cli_flow.py
================
import pytest
import subprocess
import json
from pathlib import Path

# This assumes 'aisdlc' is installed and in PATH, or you can call it via 'python -m ai_sdlc.cli'
AISDLC_CMD = ["aisdlc"]  # Or ["python", "-m", "ai_sdlc.cli"]

def run_aisdlc_command(cwd: Path, *args):
    """Helper to run aisdlc commands in tests."""
    return subprocess.run(
        AISDLC_CMD + list(args),
        capture_output=True,
        text=True,
        cwd=cwd,
        check=False  # Handle non-zero exit codes in tests
    )

@pytest.fixture
def mock_cursor_agent(mocker):
    """Mock the cursor agent subprocess call to return predictable output."""
    def _mock_cursor_agent_call(cmd_args, text=True, timeout=None):
        # cmd_args[2] is the path to the temporary prompt file
        # For simplicity, just return a fixed string.
        # A more advanced mock could read the input prompt and return step-specific content.
        try:
            prompt_content = Path(cmd_args[3]).read_text()  # Using file path from args
            if "02-prd-prompt.md" in prompt_content:
                return "# Mock PRD Content\n\n## Overview\n\nThis is a mock PRD for testing."
            if "03-prd-plus-prompt.md" in prompt_content:
                return "# Mock PRD Plus Content\n\n## Additional Details\n\nThis is a mock PRD+ for testing."
        except (IndexError, FileNotFoundError):
            pass
        
        # Default mock response
        return "# Mock Generic Content\n\nThis is a generic mock response for testing."

    return mocker.patch('subprocess.check_output', side_effect=_mock_cursor_agent_call)


def test_full_lifecycle_flow(temp_project_dir: Path, mock_cursor_agent, mocker):
    """Test the entire aisdlc workflow from init through next to done."""
    # 1. Set up required files
    # Create minimal .aisdlc config
    (temp_project_dir / ".aisdlc").write_text(
        'version = "0.1.0"\n'
        'steps = ["01-idea", "02-prd"]\n'
        'prompt_dir = "prompts"\n'
        'active_dir = "doing"\n'
        'done_dir = "done"\n'
    )
    
    # Create prompt templates
    prompts_dir = temp_project_dir / "prompts"
    prompts_dir.mkdir()
    (prompts_dir / "02-prd-prompt.md").write_text("<prev_step></prev_step>\nPRD Prompt Template")
    
    # Patch ROOT to point to our temp directory
    mocker.patch('ai_sdlc.utils.ROOT', temp_project_dir)
    
    # 2. Run init command
    result = run_aisdlc_command(temp_project_dir, "init")
    assert result.returncode == 0
    assert (temp_project_dir / "doing").exists()
    assert (temp_project_dir / "done").exists()
    
    # 3. Run new command
    idea_title = "My Test Idea"
    idea_slug = "my-test-idea"
    result = run_aisdlc_command(temp_project_dir, "new", idea_title)
    assert result.returncode == 0
    
    # Check if idea file was created
    idea_file = temp_project_dir / "doing" / idea_slug / f"01-idea-{idea_slug}.md"
    assert idea_file.exists()
    assert idea_title in idea_file.read_text()
    
    # Check lock file
    lock_file = temp_project_dir / ".aisdlc.lock"
    assert lock_file.exists()
    lock_content = json.loads(lock_file.read_text())
    assert lock_content["slug"] == idea_slug
    assert lock_content["current"] == "01-idea"
    
    # 4. Run next command
    # This relies on our mock_cursor_agent
    result = run_aisdlc_command(temp_project_dir, "next")
    assert result.returncode == 0
    
    # Check if PRD file was created
    prd_file = temp_project_dir / "doing" / idea_slug / f"02-prd-{idea_slug}.md"
    assert prd_file.exists()
    
    # Check lock file was updated
    lock_content = json.loads(lock_file.read_text())
    assert lock_content["current"] == "02-prd"
    
    # 5. Run done command
    # This would need the project to be in its final state
    # For a real test, we'd need to mock or create all expected files
    
    # For now, let's just verify the lock file structure
    assert "slug" in lock_content
    assert "current" in lock_content
    assert "created" in lock_content

================
File: tests/unit/test_init_command.py
================
import pytest
from pathlib import Path
from ai_sdlc.commands import init
from ai_sdlc import utils

def test_run_init(temp_project_dir: Path, mocker):
    mocker.patch('ai_sdlc.utils.ROOT', temp_project_dir)
    mock_write_lock = mocker.patch('ai_sdlc.utils.write_lock')
    
    # Mock any directory operations to prevent actual file system changes
    mocker.patch.object(Path, 'mkdir')
    
    init.run_init()
    
    # Verify directories would have been created
    Path.mkdir.assert_any_call(parents=True, exist_ok=True)
    
    # Verify lock file would have been written
    mock_write_lock.assert_called_once_with({})

================
File: tests/unit/test_utils.py
================
import pytest
import json
from pathlib import Path
from ai_sdlc import utils

def test_slugify():
    assert utils.slugify("Hello World!") == "hello-world"
    assert utils.slugify("  Test Slug with Spaces  ") == "test-slug-with-spaces"
    assert utils.slugify("Special!@#Chars") == "special-chars"
    assert utils.slugify("") == "idea"  # As per current implementation

def test_load_config_success(temp_project_dir: Path, mocker):
    mock_aisdlc_content = """
    version = "0.1.0"
    steps = ["01-idea", "02-prd"]
    prompt_dir = "prompts"
    """
    aisdlc_file = temp_project_dir / ".aisdlc"
    aisdlc_file.write_text(mock_aisdlc_content)

    mocker.patch('ai_sdlc.utils.ROOT', temp_project_dir)  # Ensure ROOT points to test dir

    config = utils.load_config()
    assert config["version"] == "0.1.0"
    assert config["steps"] == ["01-idea", "02-prd"]

def test_load_config_missing(temp_project_dir: Path, mocker):
    mocker.patch('ai_sdlc.utils.ROOT', temp_project_dir)
    with pytest.raises(FileNotFoundError, match="manifest missing"):
        utils.load_config()

def test_load_config_corrupted(temp_project_dir: Path, mocker):
    aisdlc_file = temp_project_dir / ".aisdlc"
    aisdlc_file.write_text("this is not valid toml content {")  # Corrupted TOML
    mocker.patch('ai_sdlc.utils.ROOT', temp_project_dir)
    mocker.patch('sys.exit')  # Prevent test suite from exiting

    # Should call sys.exit(1)
    utils.load_config()
    utils.sys.exit.assert_called_once_with(1)

def test_read_write_lock(temp_project_dir: Path, mocker):
    mocker.patch('ai_sdlc.utils.ROOT', temp_project_dir)
    lock_data = {"slug": "test-slug", "current": "01-idea"}

    # Test write_lock
    utils.write_lock(lock_data)
    lock_file = temp_project_dir / ".aisdlc.lock"
    assert lock_file.exists()
    assert json.loads(lock_file.read_text()) == lock_data

    # Test read_lock
    read_data = utils.read_lock()
    assert read_data == lock_data

    # Test read_lock when file doesn't exist
    lock_file.unlink()
    assert utils.read_lock() == {}

    # Test read_lock with corrupted JSON
    lock_file.write_text("not json {")
    assert utils.read_lock() == {}  # Should return empty dict on corruption

================
File: tests/__init__.py
================
# Empty __init__.py file for tests package

================
File: tests/conftest.py
================
import pytest
import tempfile
from pathlib import Path
import shutil

@pytest.fixture
def temp_project_dir(tmp_path: Path):
    """Creates a temporary directory simulating a project root."""
    # tmp_path is a pytest fixture providing a temporary directory unique to the test
    # For more complex setups, you might copy baseline files here
    return tmp_path

================
File: .aisdlc
================
version = "0.1.0"

# ordered lifecycle steps
steps = [
  "01-idea",
  "02-prd",
  "03-prd-plus",
  "04-architecture",
  "05-system-patterns",
  "06-tasks",
  "07-tests"
]

slug_rule    = "kebab-case"
template_dir = "templates"
prompt_dir   = "prompts"
active_dir   = "doing"
done_dir     = "done"

[mermaid]
graph = """
flowchart TD
  I[01-idea]-->P1[02-prd]-->P2[03-prd-plus]-->A[04-architecture]
  A-->SP[05-system-patterns]-->T[06-tasks]-->TESTS[07-tests]
"""

================
File: .aisdlc.lock
================
{
  "slug": "hello-world-test",
  "current": "01-idea",
  "created": "2025-05-17T19:05:28.262597"
}

================
File: .gitignore
================
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
uv.lock # Commit for consistent contributor environments

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc

# Cursor  
#  Cursor is an AI-powered code editor.`.cursorignore` specifies files/directories to 
#  exclude from AI features like autocomplete and code analysis. Recommended for sensitive data
#  refer to https://docs.cursor.com/context/ignore-files
.cursorignore
.cursorindexingignore

================
File: .pyrightconfig.json
================
{
    "typeCheckingMode": "basic",
    "pythonVersion": "3.13",
    "venvPath": ".",
    "venv": ".venv"
  }

================
File: CHANGELOG.md
================
# Changelog

All notable changes to the `ai-sdlc` project will be documented in this file.

## [0.3.0] - Unreleased

### Added
- Comprehensive testing framework with pytest
  - Basic test structure with fixtures and helpers
  - Unit tests for utility functions and commands
  - Integration tests for the CLI workflow
- Better error handling throughout the codebase
  - Detailed error messages for Cursor agent failures including stdout/stderr
  - Timeout handling for Cursor agent calls (5 minute default)
  - Comprehensive file I/O error handling in all commands
  - Lock file corruption handling (gracefully handles invalid JSON)
  - Config file corruption handling (shows helpful error for invalid TOML)

### Changed
- Improved temporary file handling in `next` command
  - Replaced hardcoded `/tmp/aisdlc_prompt.md` with secure `tempfile.NamedTemporaryFile`
  - Added proper cleanup in `finally` block to prevent leaks
- Enhanced `next` command with verbose output
  - Added informative messages about reading files, creating temporary files
  - Shows progress indicators at each step
- Improved project root detection
  - New `find_project_root()` function that searches upward for `.aisdlc` file
  - Allows running commands from subdirectories of the project

### Security
- Implemented secure temporary file handling with proper permissions

## [0.2.0] - 2025-05-17

- Python 3.13 support
- UV-first installation
- Streamlined 7-step lifecycle (removed release planning/retro steps)

## [0.1.0] - Initial Release

- Initial version with basic SDLC workflow
- Support for the 7-step lifecycle
- Integration with Cursor agent

================
File: LICENSE
================
MIT License

Copyright (c) 2025 Parker Rex

Permission is hereby granted, free of charge, to any person obtaining a copy
...

================
File: pyproject.toml
================
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#   pyproject.toml  ‚Äì  ai-sdlc
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

[build-system]
requires      = ["setuptools>=68.0", "wheel"]          # modern setuptools
build-backend = "setuptools.build_meta"

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
[project]
name            = "ai-sdlc"
version         = "0.3.0"
description     = "Markdown-driven SDLC with Cursor agent chaining."
readme          = "README.md"
authors         = [{ name = "Parker Rex" }]
license         = { text = "MIT" }
requires-python = ">=3.13.0"              # ‚¨Ö Python 3.13

[project.scripts]
aisdlc = "ai_sdlc.cli:main"

[project.optional-dependencies]
dev = [
    "pytest>=7.0",
    "pytest-mock>=3.0",
    "ruff>=0.0.292",
    "pyright>=1.1.350"
]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  Packaging tweaks
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
[tool.setuptools]
packages = ["ai_sdlc"]                    # only ship the main package

[tool.setuptools.package-data]
"ai_sdlc" = [
    "templates/*",
    "prompts/*",
    ".aisdlc"
]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  uv configuration
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
[tool.uv]
virtualenvs.in-project = true   # create .venv in repo root
sync.subprocesses      = true   # allow post-install hooks

================
File: README.md
================
# AI SDLC ‚Äî README

> **Version 0.3.0 (2025‚Äë07‚Äë21)** ‚Äî Python 3.13 ¬∑ `uv`‚Äëfirst install ¬∑ 7‚Äëstep lifecycle ¬∑ Improved error handling & testing

---

## Overview

AI SDLC is a command-line tool that orchestrates a 7-step Software Development Lifecycle (SDLC) driven by Markdown files and powered by AI agents (specifically, the Cursor editor's agent mode). It helps structure the process from idea to test planning, using AI to generate content for each step based on the previous one. This approach aims to streamline development workflows, ensure comprehensive documentation, and leverage AI for common SDLC tasks.

---

## For Users: Getting Started with `aisdlc`

This section guides you on how to use the `aisdlc` CLI tool to manage the lifecycle of your software features.

### 1. Installation

Ensure you have Python 3.13 or newer.

```bash
# Install uv (a fast Python package installer and resolver) if you haven't already:
curl -LsSf https://astral.sh/uv/install | sh

# Create a Python 3.13+ virtual environment for your project:
# (Navigate to your project directory first)
uv venv --python 3.13

# Activate your virtual environment:
# On macOS/Linux:
source .venv/bin/activate
# On Windows (PowerShell):
# .\.venv\Scripts\Activate.ps1
# On Windows (CMD):
# .\.venv\Scripts\activate.bat

# Install ai-sdlc into your activated virtual environment:
uv pip install ai-sdlc
```

### 2. Initializing Your Project

Before you can use `aisdlc` for a project, you need to initialize it. This sets up the necessary configuration and directories.

```bash
# Navigate to your project's root directory:
# cd /path/to/your-project

# Initialize ai-sdlc for this project:
aisdlc init
```
This command will create:
*   `.aisdlc`: A TOML configuration file for the AI SDLC process.
*   `prompts/`: A directory containing prompt templates for each SDLC step.
*   `doing/`: A directory where active feature workstreams will reside.
*   `done/`: A directory where completed feature workstreams will be archived.
*   `rules/`: (Potentially) A directory for evolving architecture & pattern docs (generated in later steps).

### 3. Starting a New Feature Workstream

To begin working on a new feature or idea:

```bash
aisdlc new "My Awesome Feature Idea"
```
Replace `"My Awesome Feature Idea"` with a concise title for your feature. This command:
1.  Creates a unique slug (e.g., `my-awesome-feature-idea`).
2.  Creates a new directory: `doing/my-awesome-feature-idea/`.
3.  Creates the first file: `doing/my-awesome-feature-idea/01-idea-my-awesome-feature-idea.md`.
4.  Updates `.aisdlc.lock` to track this new workstream.

You should then open the `01-idea-*.md` file in your editor (like Cursor) and fill out the "Problem," "Solution," and "Rabbit Holes" sections.

### 4. Advancing Through the SDLC

Once you've completed a step's Markdown file (e.g., after filling out `01-idea-*.md`), you can advance to the next step:

```bash
aisdlc next
```
This command:
1.  Reads the current state from `.aisdlc.lock`.
2.  Identifies the previous step's Markdown file (e.g., `01-idea-*.md`).
3.  Identifies the prompt template for the *next* step (e.g., `prompts/02-prd-prompt.md`).
4.  Merges the content of the previous step's file into the prompt template.
5.  Invokes the Cursor agent with this merged prompt.
6.  Saves the AI-generated output to the new step's Markdown file (e.g., `doing/my-awesome-feature-idea/02-prd-my-awesome-feature-idea.md`).
7.  Updates `.aisdlc.lock` to the new current step.

You then review and edit the newly generated file, and repeat `aisdlc next` to proceed through all 7 steps.

### 5. Checking Status and Finishing

At any point, you can check the status of your active workstream:
```bash
aisdlc status
```
This will show the current feature slug, the current step, and a progress bar.

Once all 7 steps are completed and you are satisfied with the `07-tests-*.md` file:
```bash
aisdlc done
```
This command:
1.  Validates that all 7 Markdown files for the workstream exist.
2.  Moves the feature directory from `doing/` to `done/`.
3.  Clears the `.aisdlc.lock` file, making the system ready for a new workstream.

### CLI Command Reference

| Command                     | Purpose                                                                  |
| --------------------------- | ------------------------------------------------------------------------ |
| `aisdlc init`               | Scaffolds `.aisdlc`, `prompts/`, `doing/`, `done/` if missing.            |
| `aisdlc new "<idea title>"` | Creates a new workstream folder and the initial `01-idea-*.md` file.     |
| `aisdlc next`               | Reads lock ‚ûú picks next prompt ‚ûú calls Cursor ‚ûú writes file ‚ûú updates lock. |
| `aisdlc status`             | Shows the active workstream and its progress through the lifecycle steps.  |
| `aisdlc done`               | Validates all 7 files exist for the current workstream ‚ûú moves folder to `done/`. |

### The 7-Step Lifecycle

The AI SDLC process follows these ordered steps:

| Step | File suffix       | What you (and the AI) produce                                 |
| ---- | ----------------- | ------------------------------------------------------------- |
| 01   | `idea`            | Problem statement, solution sketch, potential rabbit holes.   |
| 02   | `prd`             | Core product requirements, user stories, acceptance criteria. |
| 03   | `prd-plus`        | Critical evaluation of PRD, risks, KPIs, UX considerations.   |
| 04   | `architecture`    | Analysis of current (if any) and proposed file structure, system patterns. |
| 05   | `system-patterns` | Canonical documentation for system architecture, design patterns, tech stack. |
| 06   | `tasks`           | Granular, actionable, ID-tagged development tasks.            |
| 07   | `tests`           | Unit, integration, and behavioral/E2E test plans and scenarios. |

*(Steps 08 & 09 for release planning/retro were removed in v0.2 ‚Äì these are now expected to live in your preferred ticket tracker.)*

---

## For Developers: Contributing to `ai-sdlc`

This section is for those who want to contribute to the development of the `ai-sdlc` tool itself.

### 1. Prerequisites

| Tool                              | Why we chose it                                             | Install (macOS example)                                       |
| --------------------------------- | ----------------------------------------------------------- | ------------------------------------------------------------- |
| **uv**                            | Fast, deterministic builds, manages virtual-envs in-project | `curl -LsSf https://astral.sh/uv/install \| sh`               |
| **Python 3.13.0** (or newer)      | Modern language features (e.g., `tomllib`) & performance    | `uv venv --python=3.13` (or `pyenv install 3.13.0`)           |
| **Node ‚â• 20 / pnpm** *(optional)* | If you plan to extend any TypeScript helpers (if present)   | `brew install node pnpm`                                      |
| **Cursor Editor**                 | Required for the `aisdlc next` command's agent functionality| Download from [cursor.sh](https://cursor.sh/)                 |

### 2. Development Setup

```bash
# Clone the repository:
git clone https://github.com/USERNAME/ai-sdlc.git # Replace USERNAME/ai-sdlc with the actual repo URL
cd ai-sdlc

# Create/activate Python 3.13+ virtual environment using uv:
uv venv # This will use the Python version specified in pyproject.toml or a globally available 3.13+
# Activate the environment:
# On macOS/Linux:
source .venv/bin/activate
# On Windows (PowerShell):
# .\.venv\Scripts\Activate.ps1

# Install dependencies, including development tools, from the lock file:
uv sync --all-features # Or uv sync --features dev if 'dev' is an optional feature

# Alternatively, if you've changed pyproject.toml and need to update the lock file:
# uv pip install -e .[dev] # Installs in editable mode with dev dependencies
# uv lock # Updates uv.lock based on pyproject.toml

# Set up pre-commit hooks (optional but recommended):
# uv pip install pre-commit
# pre-commit install
```

### 3. Running Linters and Tests

Ensure development dependencies are installed (via `uv sync --all-features` or `uv pip install -e .[dev]`).

```bash
# Run Ruff linter/formatter:
uv run ruff check ai_sdlc tests
uv run ruff format ai_sdlc tests

# Run Pyright type checker:
uv run pyright

# Run Pytest test suite:
uv run pytest
```
If TypeScript utilities are part of the project (e.g., in a `ts/` subdirectory), navigate to those packages and use `pnpm lint` / `pnpm test` as configured there.

### 4. Project Structure

*   `ai_sdlc/`: The main Python package for the CLI tool.
    *   `cli.py`: Entry point for the `aisdlc` command.
    *   `commands/`: Modules for each subcommand (e.g., `new.py`, `next.py`).
    *   `utils.py`: Shared helper functions.
*   `prompts/`: Contains Markdown templates used by the AI agent for each SDLC step. These are critical to the tool's behavior.
*   `tests/`: Contains unit and integration tests for the `ai-sdlc` tool.
*   `.aisdlc`: The TOML configuration file defining lifecycle steps, directory names, etc. This is scaffolded by `aisdlc init`.
*   `.aisdlc.lock`: A JSON file tracking the state of the currently active workstream (slug, current step). **Do not edit manually.**
*   `.cursor/rules/ai_sdlc.mdc`: Defines rules for how the Cursor AI agent should behave when interacting with this project.

### 5. Understanding the Core Logic

1.  The `aisdlc` command (from `cli.py`) parses the subcommand and arguments.
2.  It dynamically imports and calls the appropriate `run_*` function from the `ai_sdlc.commands` module.
3.  Commands typically:
    *   Load configuration from `.aisdlc` (via `utils.load_config()`).
    *   Read the current workstream state from `.aisdlc.lock` (via `utils.read_lock()`).
    *   Perform file system operations (creating/moving files and directories).
    *   For `aisdlc next`, it constructs a prompt by merging the previous step's output with a template from `prompts/` and calls the `cursor agent` via a subprocess.
    *   Update `.aisdlc.lock` (via `utils.write_lock()`).

### 6. Cursor Agent Interaction

The `aisdlc next` command is central to the AI-driven workflow.
*   It relies on the Cursor editor's "agent" mode being available in the system's PATH.
*   The file `.cursor/rules/ai_sdlc.mdc` provides high-level instructions to the Cursor agent on how to operate within an AI-SDLC workstream.
*   Each prompt template in `prompts/` (e.g., `02-prd-prompt.md`) contains a placeholder: `<prev_step></prev_step>`.
*   Before calling the agent, `aisdlc next` replaces this placeholder with the full Markdown content of the *previous* completed step. This provides context to the AI for generating the *current* step.

### 7. Contributing Guidelines

*   **Branching:** Create a new branch for each feature or bug fix (e.g., `feature/new-command`, `fix/readme-typo`).
*   **Commits:** Write clear and concise commit messages.
*   **Code Style:** Follow PEP 8 guidelines. Use Ruff for linting and formatting.
*   **Testing:** Add tests for any new functionality or bug fixes. Ensure all tests pass before submitting a pull request.
*   **Pull Requests:** Provide a clear description of the changes in your PR. If it addresses an issue, link to it.

---

## Lifecycle Mermaid (auto‚Äërendered from `.aisdlc`)

This diagram illustrates the flow of the 7-step lifecycle:
```mermaid
flowchart TD
  I[01‚Äëidea]-->P1[02‚Äëprd]-->P2[03‚Äëprd‚Äëplus]-->A[04‚Äëarchitecture]
  A-->SP[05‚Äësystem‚Äëpatterns]-->T[06‚Äëtasks]-->TESTS[07‚Äëtests]
```

---

## Roadmap Ideas
*   [ ] Adding a step for deployment/infrastructure (CI/CD).
*   [ ] Think about repomix as part of the flow.
*   [ ] Add prompt for actually doing the tasks (should check against the token limit).
*   [ ] Preferred technologies for the project (Python, TypeScript, etc.).
*   [ ] More self improvement prompts for patterns in the codebase.
*   [ ] Re-evaluate incorporating a "08-release-plan" step for outlining deployment strategy, versioning, and communication.
*   [ ] Flexible AI Model Integration: Move away from hardcoding `cursor agent`. Allow users to specify their preferred AI model via a CLI flag (e.g., `--model gpt-4o`, `--model claude-3-opus`). This would involve users providing their own API keys (e.g., via environment variables configured in `.env` based on an updated `.env.example`), allowing separate model choices for planning/documentation steps vs. code generation steps.
*   [ ] Context Window Management: Implement strategies for managing large context windows, potentially drawing inspiration from tools like `aider` or `mentat`. This could involve techniques like summarization, embedding-based retrieval for relevant context, or more sophisticated context chunking when interacting with LLMs.

---

*Made with ‚òï by Parker Rex & community. MIT License.*

================
File: RELEASE_NOTES.md
================
# AI-SDLC Release Notes - v0.3.0

## Overview

Version 0.3.0 of AI-SDLC brings significant improvements to error handling, security, and overall robustness of the tool. This release also introduces a comprehensive testing framework to ensure stability as the project evolves.

## Key Improvements

### Enhanced Error Handling and Robustness

- **Improved Cursor Agent Error Handling**: You now get detailed error messages when Cursor agent calls fail, including stdout and stderr output for better debugging.
- **Timeout Protection**: A 5-minute timeout has been added to Cursor agent calls to prevent indefinite hangs.
- **Comprehensive File I/O Error Handling**: All file operations now have proper exception handling with informative error messages.
- **Configuration File Validation**: The tool now gracefully handles corrupted TOML configuration files with clear error messages.
- **Lock File Protection**: Corrupted JSON in the lock file is now handled gracefully, treating it as empty with a warning.

### Secure Temporary File Management

- **Secure Temporary Files**: Replaced hardcoded `/tmp/aisdlc_prompt.md` with the secure `tempfile.NamedTemporaryFile` mechanism.
- **Automatic File Cleanup**: Added proper cleanup in `finally` blocks to prevent leaking sensitive prompt data.

### Improved Project Navigation

- **Robust Project Root Detection**: You can now run `aisdlc` commands from any subdirectory within your project.
- **Upward Directory Search**: The tool automatically searches parent directories to find the project root (marked by the `.aisdlc` file).

### Enhanced User Experience

- **Verbose Output**: The `next` command now provides detailed progress information at each step.
- **Clear Error Messages**: All error messages have been improved to be more informative and actionable.

### Testing Infrastructure

- **Comprehensive Test Suite**: Added a full testing framework using pytest.
- **Unit Tests**: Core utility functions and command modules now have dedicated unit tests.
- **Integration Tests**: End-to-end workflow tests ensure commands work together correctly.
- **Test Fixtures**: Reusable fixtures for mocking and temporary project environments.

## Developer Improvements

- **Development Dependencies**: Added pytest, pytest-mock, ruff, and pyright to the dev dependencies.
- **Documentation Updates**: Improved documentation for development setup and testing.

## Upgrading

To upgrade to version 0.3.0, use:

```bash
uv pip install --upgrade ai-sdlc
```

No configuration changes are needed - all improvements are backward compatible with existing AI-SDLC projects.

## Known Issues

- The `cursor agent` command must still be available in your PATH for the `next` command to work correctly.
- AI-SDLC continues to require Python 3.13 or newer.

## Future Directions

We're exploring:
- Multiple AI model provider support beyond Cursor
- Context window management for larger projects
- Extended lifecycle steps for deployment and infrastructure
- See the Roadmap section in README.md for more planned improvements

---

Thank you to all contributors who helped make this release possible!

================
File: uv.lock
================
version = 1
revision = 2
requires-python = ">=3.13.0"

[[package]]
name = "ai-sdlc"
version = "0.1.0"
source = { editable = "." }
